# 🎬 神经网络入门：天气预测 —— 逐字讲稿

> 📝 配合 `forward-weather.py` 代码使用
> 预计时长：40-50 分钟

---

## 开场白（2分钟）

大家好！今天我们来学习神经网络的入门知识。

很多同学一听到"神经网络"就觉得很高大上、很复杂。其实没那么可怕！我们今天就用一个简单的例子——**预测天气会不会下雨**——来带你入门。

学完这节课，你会掌握三个核心技能：
1. 理解什么是前馈神经网络
2. 会用 PyTorch 搭建一个简单的神经网络
3. 理解训练的完整流程

准备好了吗？我们开始！

---

## 第〇部分：神经网络全景图（5分钟）

在开始写代码之前，我们先花几分钟了解一下神经网络的整体概念。这样在后面学习具体代码时，你会更清楚每一步在做什么。

### 🧠 什么是神经网络？

一句话：**神经网络是一种"函数逼近器"**。

啥意思？就是你给它输入 x，它给你输出 y。中间怎么算的？它自己学出来。

```
输入 x  ──►  [ 神经网络（黑盒子）]  ──►  输出 y
```

传统编程：你告诉计算机"怎么算"（写规则）
神经网络：你告诉计算机"答案是什么"（给数据），它自己学"怎么算"

🧠 **生活比喻**：

传统编程就像教小孩背乘法口诀：三七二十一，四七二十八...
神经网络就像让小孩看很多例子：3×7=21，4×7=28...然后它自己悟出规律。

### 📊 神经网络的"家族"

神经网络有很多种，就像汽车有轿车、SUV、卡车一样，各有各的用途：

| 类型 | 全称 | 擅长领域 | 特点 |
|------|------|----------|------|
| **MLP/前馈网络** | Multi-Layer Perceptron | 表格数据、简单分类 | 最基础，今天学这个 |
| **CNN** | Convolutional Neural Network | 图像识别 | 能"看图"，下节课学 |
| **RNN** | Recurrent Neural Network | 序列数据（文本、时间序列） | 有"记忆"能力 |
| **Transformer** | - | NLP、大模型 | ChatGPT 的基础 |
| **GAN** | Generative Adversarial Network | 生成图像 | 能"画画" |

今天我们学的是**最基础的前馈神经网络（MLP）**，也叫全连接网络。它是所有其他网络的基础，学会了它，其他的都好理解。

### 🎯 机器学习的三种"学习方式"

神经网络属于机器学习的一种。机器学习有三种主要的学习方式：

**1️⃣ 监督学习（Supervised Learning）—— 今天用的！**

- 特点：有"标准答案"
- 比喻：老师批改作业，告诉你对错
- 例子：给一堆（天气特征 → 下雨/不下雨）的数据，让网络学习
- 今天的代码就是这种！

**2️⃣ 无监督学习（Unsupervised Learning）**

- 特点：没有标准答案
- 比喻：自己看书，自己总结规律
- 例子：给一堆用户数据，让网络自动分成几类
- 典型算法：聚类、降维、自编码器

**3️⃣ 强化学习（Reinforcement Learning）**

- 特点：通过"奖惩"学习
- 比喻：训练小狗，做对了给零食，做错了不给
- 例子：训练 AI 玩游戏、下围棋
- 典型应用：AlphaGo、机器人控制

📊 **三种学习方式对比图**：

```
监督学习：    数据 + 标签  ──►  学习  ──►  预测
             (x, y)

无监督学习：  数据（无标签）──►  学习  ──►  发现结构
             (x)

强化学习：    状态 ──► 动作 ──► 奖励 ──► 学习 ──► 更好的动作
             (循环)
```

🔑 **记住**：今天我们用的是**监督学习**。我们有输入（云量、风速、湿度）和标签（下雨/不下雨），让网络学习它们之间的关系。

### 🔄 训练的本质是什么？

不管是哪种神经网络，训练的本质都是一样的：

**调整权重，让预测更准。**

具体怎么调？四步走（今天会详细讲）：

```
┌─────────────────────────────────────────────────────┐
│                    训练四部曲                        │
│                                                     │
│   1. 前向传播：用当前权重算出预测值                   │
│          ↓                                          │
│   2. 计算损失：预测值和真实值差多远                   │
│          ↓                                          │
│   3. 反向传播：算出每个权重该怎么调（梯度）            │
│          ↓                                          │
│   4. 更新权重：根据梯度调整权重                       │
│          ↓                                          │
│      重复以上步骤，直到损失足够小                     │
└─────────────────────────────────────────────────────┘
```

这个过程就像：
- 前向传播 = 学生答题
- 计算损失 = 老师打分
- 反向传播 = 分析哪里错了
- 更新权重 = 改正错误认知

好，有了这个全景图，我们来看具体代码！


![alt text](<截屏2025-12-14 23.44.41.png>)


---

## 第一部分：生活化理解神经网络（3分钟）

在写代码之前，我先打个比方，帮你建立直觉。

**神经网络就像一个"学徒厨师"：**

- **输入数据**，就是食材。今天我们的食材是：云量、风速、湿度。
- **网络的权重**，就是厨师的烹饪经验。刚开始学徒啥也不会，权重是随机的。
- **输出结果**，就是做出来的菜——我们这里是"下雨"还是"不下雨"的预测。
- **训练过程**，就是师傅不断纠正学徒。做得不好，师傅说哪里错了，学徒改正，慢慢就学会了。

记住这个比喻，后面遇到抽象概念时，回来想想这个厨师的故事。

---

## 第二部分：导入库（2分钟）

好，我们来看代码。

```python
import torch              
import torch.nn as nn     
import torch.optim as optim  
import numpy as np        
```

首先是导入库。每个库就像一个"工具箱"：

**torch** —— 这是 PyTorch 的核心库，可以说是深度学习的"瑞士军刀"，什么都能干。

**torch.nn** —— nn 是 Neural Network 的缩写，神经网络模块。里面有各种"积木"，比如全连接层、激活函数等等。我们搭网络就靠它。

**torch.optim** —— optim 是 Optimizer 的缩写，优化器模块。它负责"调整权重"，让网络学得更好。

**numpy** —— 数值计算库，用来生成和处理数据。虽然 PyTorch 也能做，但 NumPy 用起来更方便。

---

## 第三部分：随机种子（2分钟）

```python
torch.manual_seed(42)     
np.random.seed(42)        
```

这两行是设置随机种子。

❓ **等等，为什么会有"随机"这回事？哪里用到随机了？**

好问题！在我们这个程序里，至少有两个地方用到了随机：

**第一个地方：生成训练数据**

```python
cloud = np.random.randint(0, 100, num_samples)  # 随机生成云量
wind = np.random.randint(0, 100, num_samples)   # 随机生成风速
humidity = np.random.randint(0, 100, num_samples)  # 随机生成湿度
```

每次运行，生成的 500 条天气数据都不一样。今天可能生成云量是 [51, 92, 14, ...]，明天再运行可能就是 [23, 67, 88, ...]。

**第二个地方：神经网络权重初始化**

```python
self.fc1 = nn.Linear(3, 8)  # 这里面的权重是随机初始化的！
self.fc2 = nn.Linear(8, 1)  # 这里也是！
```

你可能没注意到，当我们创建 `nn.Linear` 层时，PyTorch 会**随机初始化**里面的权重 W 和偏置 b。

🧠 **为什么权重要随机初始化？**

打个比方：如果所有神经元的权重都一样（比如都是 0 或都是 1），那它们就像一群"复制人"，学到的东西完全一样，相当于只有一个神经元在干活。

随机初始化让每个神经元"个性不同"，各自分工，才能学到不同的特征。

❓ **那为什么要设置随机种子？**

打个比方：随机种子就像是游戏里的"存档点"。

- 没有种子：每次运行结果都不一样，就像每次玩游戏都从头开始，走的路线都不同。
- 有了种子：每次运行结果一致，就像从存档点继续，一切都一样。

🧠 **更技术一点的解释**：

计算机里的"随机数"其实是"伪随机数"——它是用一个数学公式算出来的，看起来像随机，但其实是确定的。

这个公式需要一个"起点"，就是种子（seed）。同样的种子，算出来的随机数序列完全一样。

```
种子 42 → 随机序列：0.37, 0.95, 0.73, 0.59, ...
种子 42 → 随机序列：0.37, 0.95, 0.73, 0.59, ...  (完全一样！)
种子 123 → 随机序列：0.69, 0.28, 0.43, 0.55, ... (不一样了)
```

这在调试的时候特别重要！你改了一行代码，想看效果变了没有，如果每次运行结果都随机，你怎么知道是代码的问题还是随机性的问题？

💡 顺便说一句，42 这个数字是个"梗"，来自科幻小说《银河系漫游指南》，代表"生命、宇宙以及一切的答案"。所以很多程序员喜欢用 42 做种子。

---

## 第四部分：生成训练数据（5分钟）

```python
num_samples = 500  
```

我们要生成 500 条"假"天气记录。

为什么是"假"的？因为真实世界里，你得去气象站收集几十年的数据。我们这里是教学，用规则模拟就行了，重点是理解原理。

500 这个数量怎么选的？**太少学不好，太多训练慢**。500 是入门练习的合适数量。

### 生成输入特征

```python
cloud = np.random.randint(0, 100, num_samples)     
wind = np.random.randint(0, 100, num_samples)      
humidity = np.random.randint(0, 100, num_samples)  
```

想象成气象站的三个传感器：
- **cloud 云层厚度**：0 表示晴空万里，100 表示乌云密布
- **wind 风力大小**：0 表示无风，100 表示狂风大作
- **humidity 空气湿度**：0 表示干燥，100 表示潮湿

`np.random.randint(0, 100, 500)` 就是生成 500 个 0 到 99 之间的随机整数。

### 生成标签（下雨规则）

```python
labels = ((humidity + cloud) > wind * 1.2).astype(np.float32)
```

🔑 **这是我们"编造"的下雨规则：湿度 + 云量 > 风速 × 1.2 时，下雨。**

为什么这么设计？符合生活直觉：
- 湿度高、云多 → 容易下雨 ✅
- 风大 → 把云吹散，不容易下雨 ✅
- 1.2 是个"经验系数"，让规则更有区分度

⚠️ **注意**：真实天气预测比这复杂一万倍！这只是教学示例。

`.astype(np.float32)` 是把布尔值转成浮点数：True 变成 1.0，False 变成 0.0。

### 组装数据

```python
X = np.stack([cloud, wind, humidity], axis=1).astype(np.float32)
y = labels.reshape(-1, 1)  
```

**np.stack** 是把三个数组"堆叠"成一个矩阵。

`axis=1` 表示沿着"列"方向堆叠。结果是一个 (500, 3) 的矩阵：500 个样本，每个有 3 个特征。

**reshape(-1, 1)** 是把标签变成 (500, 1) 的形状。

❓ **为什么要 (500, 1) 而不是 (500,)？**

因为神经网络的输出是 (N, 1) 的形状，我们的标签形状要匹配，否则计算损失时会出错。

### 转换为 Tensor

```python
X_tensor = torch.from_numpy(X)  
y_tensor = torch.from_numpy(y)  
```

🔑 **Tensor 是什么？**

Tensor 是 PyTorch 中最核心的数据结构。它和 NumPy 数组很像，但有几个非常重要的特性：

**特性一：可以在 GPU 上运算**

GPU（显卡）有几千个小核心，擅长并行计算。神经网络的矩阵运算正好适合 GPU。

```python
# 把数据搬到 GPU 上
X_tensor = X_tensor.to('cuda')  
model = model.to('cuda')
# 之后的计算会在 GPU 上进行，速度快几十倍
```

NumPy 只能在 CPU 上算，没法利用 GPU 的算力。

**特性二：支持自动求导（Autograd）**

这是 Tensor 最强大的能力，也是深度学习框架存在的意义。

还记得训练四部曲吗？其中"反向传播"需要计算梯度：

```python
loss.backward()  # 自动计算所有参数的梯度
```

这个"自动"是怎么实现的？

Tensor 在计算时会默默"记账"——记录每一步运算是怎么来的（叫做计算图）。等你调用 `.backward()` 时，它就能沿着这个记录，用链式法则反向算出所有梯度。

🧠 **打个比方**：

- NumPy 数组像普通计算器，按完就忘
- PyTorch Tensor 像带历史记录的计算器，每一步都记着，随时可以回溯

**特性三：记录数据类型和形状**

Tensor 严格管理数据类型（dtype）和形状（shape）：

```python
X_tensor.dtype   # torch.float32
X_tensor.shape   # torch.Size([500, 3])
```

这在调试时非常有用。维度不匹配是新手最常遇到的错误，Tensor 会明确告诉你哪里出了问题。

**特性四：支持广播机制（Broadcasting）**

和 NumPy 一样，Tensor 支持不同形状的张量进行运算：

```python
# (500, 3) + (3,) 会自动广播成 (500, 3) + (500, 3)
X_tensor + torch.tensor([1, 2, 3])
```

---

❓ **为什么要转成 Tensor，不能直接用 NumPy 数组吗？**

不能。原因有两个：

**第一，PyTorch 的神经网络只认 Tensor。**

`nn.Linear`、`nn.BCELoss` 这些 PyTorch 组件，内部都是用 Tensor 做计算的。你传 NumPy 数组进去，它会报错。

**第二，也是更重要的：需要自动求导。**

NumPy 数组没有"记账"能力，无法支持反向传播。

---

**`torch.from_numpy()` 做了什么？**

它把 NumPy 数组转成 Tensor，而且是**共享内存**的——修改其中一个，另一个也会变。这样转换几乎不花时间，也不额外占内存。

```python
# 共享内存的例子
X[0, 0] = 999          # 修改 NumPy 数组
print(X_tensor[0, 0])  # Tensor 也变成 999 了！
```

如果不想共享，可以用 `torch.tensor(X)`，这会复制一份数据。

---

**🔑 深拷贝 vs 浅拷贝（重要！）**

这是编程中的一个重要概念，不只是 PyTorch，很多地方都会遇到：

| 方式 | 是否复制数据 | 内存占用 | 修改是否互相影响 |
|------|------------|---------|----------------|
| `torch.from_numpy(X)` | ❌ 不复制（浅拷贝） | 共享，省内存 | ✅ 会互相影响 |
| `torch.tensor(X)` | ✅ 复制（深拷贝） | 翻倍，各占一份 | ❌ 不会互相影响 |
| `X_tensor.clone()` | ✅ 复制（深拷贝） | 翻倍，各占一份 | ❌ 不会互相影响 |

🧠 **打个比方**：

- **浅拷贝**（`from_numpy`）：就像两个人共用一个银行账户，一个人取钱，另一个人余额也变少
- **深拷贝**（`tensor`）：就像把钱分成两份，各存各的账户，互不影响

❓ **什么时候用哪个？**

- **用 `from_numpy()`**：数据量大、确定不会修改原数据时，省内存
- **用 `tensor()`**：需要保护原数据、或者要把数据搬到 GPU 时

⚠️ **一个常见的坑**：

```python
# 危险！from_numpy 创建的 Tensor 不能直接搬到 GPU
X_tensor = torch.from_numpy(X)
X_tensor = X_tensor.to('cuda')  # 这会报错或行为异常！

# 安全做法：先 clone 再搬
X_tensor = torch.from_numpy(X).clone().to('cuda')
# 或者直接用 tensor()
X_tensor = torch.tensor(X, device='cuda')
```

因为 GPU 内存和 CPU 内存是分开的，共享内存的 Tensor 没法直接搬。

💡 **小技巧**：在实际项目中，如果数据不是特别大，建议用 `torch.tensor()` 更安全，避免意外修改原数据或 GPU 迁移问题。

---

## 第五部分：定义神经网络（8分钟）

🔑 **这是整节课最核心的部分！认真听！**

```python
class WeatherNet(nn.Module):
```

我们定义一个类，叫 `WeatherNet`，继承自 `nn.Module`。

❓ **为什么要继承 nn.Module？**

三个好处：
1. 自动追踪所有可学习的参数
2. 提供 `.parameters()` 方法，优化器需要用
3. 支持 `.to(device)` 切换 CPU 和 GPU

### __init__ 方法

```python
def __init__(self):
    super(WeatherNet, self).__init__()  
    self.fc1 = nn.Linear(3, 8)    
    self.fc2 = nn.Linear(8, 1)    
    self.sigmoid = nn.Sigmoid()   
```

`super().__init__()` 是调用父类的初始化，**必须写！** 相当于告诉 PyTorch："我要开始定义网络了"。

**nn.Linear(3, 8)** 是全连接层。

- fc 是 Fully Connected 的缩写，全连接的意思
- 3 是输入特征数（云量、风速、湿度）
- 8 是隐藏层神经元数

🧠 **什么叫"全连接"？** 就是前一层的每个神经元，都连到后一层的每个神经元。

数学公式：**y = x @ W + b**
- W 是权重矩阵，形状 (3, 8)
- b 是偏置向量，形状 (8,)

**nn.Linear(8, 1)** 是第二层：8 个输入，1 个输出。

⚠️ **注意**：第二层的输入必须是 8，和第一层的输出匹配！这是新手常犯的错。

**nn.Sigmoid()** 是 Sigmoid 激活函数，把任意数值"压缩"到 0~1 之间，变成概率。

💡 **为什么隐藏层用 8 个神经元？**
- 太少（比如 2 个）：学习能力不足，欠拟合
- 太多（比如 100 个）：可能过拟合，也更慢
- 8 是个平衡点，适合这个简单问题

这个数字叫"超参数"，需要自己调试。

### forward 方法

```python
def forward(self, x):
    x = self.fc1(x)           
    x = torch.relu(x)         
    x = self.fc2(x)           
    x = self.sigmoid(x)       
    return x  
```

**forward 方法定义数据怎么从输入流向输出，叫"前向传播"。**

🧠 打个比方：**工厂流水线**。

原材料 x 进来 → 第一道工序 fc1 + ReLU → 第二道工序 fc2 + Sigmoid → 成品出来

**第一层**：`x = self.fc1(x)` 做线性变换，然后 `torch.relu(x)` 做 ReLU 激活。

🔑 **为什么需要激活函数？**

这是个关键问题！

如果没有激活函数，多层线性变换叠加起来，还是线性变换。**线性的线性还是线性！**

有了激活函数，网络才能学习"非线性"关系。现实世界的问题大多是非线性的。

**ReLU 是什么？** 特别简单：负数变 0，正数不变。数学公式：ReLU(x) = max(0, x)

❓ **为什么 ReLU 这么简单却很有效？**
1. 计算快——只是比较和取值
2. 缓解梯度消失问题
3. 产生稀疏激活（很多 0），类似生物神经元

**第二层**：`x = self.fc2(x)` 再做一次线性变换，`self.sigmoid(x)` 压缩到 0~1 变成概率。

### 创建网络实例

```python
model = WeatherNet()
```

这行代码创建了网络实例。打印出来可以看到结构：

```
WeatherNet(
  (fc1): Linear(in_features=3, out_features=8, bias=True)
  (fc2): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
```

💡 **参数数量怎么算？**
- fc1：3×8 + 8 = 32 个（权重 24 个，偏置 8 个）
- fc2：8×1 + 1 = 9 个（权重 8 个，偏置 1 个）
- 总共：41 个参数

才 41 个参数！但就这么简单的网络，已经能学会我们的下雨规则了。

---

## 第六部分：损失函数和优化器（4分钟）

```python
criterion = nn.BCELoss()  
optimizer = optim.Adam(model.parameters(), lr=0.01)
```

🧠 **生活比喻**：
- **损失函数** = 老师的评分标准，告诉你"错得多离谱"
- **优化器** = 学习方法，告诉你"怎么根据错误来改进"

### 损失函数：BCE

BCE 是 Binary Cross Entropy，二元交叉熵。

❓ **为什么用 BCE 而不是 MSE（均方误差）？**

MSE 的公式是 $(预测值 - 真实值)^2$。

问题在哪？当预测是 0.9，真实是 1 时，MSE = 0.01，好像还行。但对于分类问题，0.9 已经很接近 1 了，不应该惩罚太多。

BCE 的公式是 $-[y \cdot \log(p) + (1-y) \cdot \log(1-p)]$

- 当真实=1，预测=0.9 时：BCE ≈ 0.1（较小惩罚）
- 当真实=1，预测=0.1 时：BCE ≈ 2.3（较大惩罚）

💡 **交叉熵的直觉：惩罚"自信的错误"**。你越自信地猜错，惩罚越重。

### 优化器：Adam

Adam 是 Adaptive Moment Estimation，自适应矩估计。

基本公式：**新权重 = 旧权重 - 学习率 × 梯度**

❌ **普通 SGD 的问题**：
- 学习率固定，有时太大有时太小
- 所有参数用同样的学习率

✅ **Adam 的优点**：
- 自适应调整每个参数的学习率
- 大多数情况下"开箱即用"

🧠 **形象比喻**：
- SGD：骑自行车下山，可能绕远路
- Momentum：滑雪下山，有惯性，更顺滑
- Adam：自动驾驶下山，智能调整方向和速度

**lr=0.01** 是学习率。意思是每次更新时，权重最多变化原来的 1%。
- 太大（如 0.5）：震荡，不收敛
- 太小（如 0.0001）：收敛太慢

0.01 是常用的起始值。

---

## 第七部分：训练循环（10分钟）

🔑 **这是理解深度学习的关键！**

```python
epochs = 50  
```

**epoch** 是什么？把所有数据过一遍，叫一个 epoch。

❓ **为什么要多个 epoch？**

一遍记不住！就像复习，多看几遍才能记牢。

### 训练四部曲

每个 epoch 都做四件事：

```
前向传播 → 计算损失 → 反向传播 → 更新权重
```

🧠 **生活比喻：学生考试**
1. 前向传播 = 学生答题
2. 计算损失 = 老师打分，看错了多少
3. 反向传播 = 分析错误原因，哪里理解错了
4. 更新权重 = 改正错误认知

### 步骤 1：前向传播

```python
outputs = model(X_tensor)  
```

把数据"喂"给网络，得到预测结果。

💡 这里 `model(X_tensor)` 实际上会调用 `model.forward(X_tensor)`。PyTorch 的魔法：直接 `model(x)` 等价于 `model.forward(x)`。

### 步骤 2：计算损失

```python
loss = criterion(outputs, y_tensor)
```

比较预测值和真实值，计算"错了多少"。

loss 是一个标量，单个数值。越小越好，0 表示完美预测。

### 步骤 3：梯度清零

```python
optimizer.zero_grad()
```

⚠️ **这步很容易忘！** 新手常见 bug。

❓ **为什么要清零？**

PyTorch 的设计：梯度默认是**累加**的。有些算法需要这个特性。

但普通训练：每次迭代应该用新梯度，所以要先清零。

❌ **忘记清零的后果**：梯度越来越大，权重更新越来越剧烈，网络"爆炸"。

### 步骤 4：反向传播

```python
loss.backward()
```

🔑 **这是深度学习的核心！**

这一行代码自动计算每个参数的梯度（偏导数）。

🧠 **数学原理：链式法则**（高中学过）

如果 $z = f(y)$，$y = g(x)$，那么 $\frac{dz}{dx} = \frac{dz}{dy} \times \frac{dy}{dx}$

神经网络就是很多层函数嵌套，PyTorch 自动帮你用链式法则算出所有梯度。

你不用手动算！这就是 PyTorch 的强大之处。

### 步骤 5：更新权重

```python
optimizer.step()
```

根据梯度调整参数。

本质上做的事：$w = w - lr \times \frac{\partial loss}{\partial w}$

但 Adam 会更复杂一些，它会自适应调整学习率。

### 记录和打印

```python
loss_history.append(loss.item())  
```

`.item()` 把 tensor 变成 Python 数字，方便记录。

```python
if (epoch + 1) % 5 == 0:
    # 打印进度...
```

每 5 轮打印一次，看看训练效果。

你会看到 Loss 从 0.6 左右逐渐下降到 0.2 左右，准确率从 60% 左右上升到 90% 以上。

这就是神经网络在"学习"！

---

## 第八部分：测试模型（3分钟）

```python
with torch.no_grad():
```

**torch.no_grad()** 告诉 PyTorch："我只是在预测，不需要计算梯度"。

好处：
1. 省内存（不用存储中间值）
2. 计算更快

### 测试样本

```python
test_sample = torch.tensor([[80, 10, 70]], dtype=torch.float32)
```

云=80（很厚的云），风=10（微风），湿=70（比较潮湿）。

根据我们的规则：70 + 80 = 150 > 10 × 1.2 = 12，应该下雨。

```python
prediction = model(test_sample).item()  
```

神经网络给出的预测概率应该接近 1（大概 0.95 以上），说明它学会了！

### 多测几个样本

代码里还测了几个不同的情况：
- 云少、风大、干燥 → 应该不下雨
- 云厚、风小、潮湿 → 应该下雨
- 中等情况 → 边界

你会发现神经网络的预测和规则基本一致，说明它确实学会了我们编造的规则。

---

## 总结（2分钟）

好，我们来回顾一下今天学了什么。

### 🔑 核心知识点

**1. 神经网络 = 层的堆叠**

每层做两件事：线性变换 + 非线性激活。没有激活函数，再多层也等于一层。

**2. 训练四部曲**

前向传播 → 计算损失 → 反向传播 → 更新权重

**3. 常见坑**
- ⚠️ 忘记 `optimizer.zero_grad()`
- ⚠️ 输入输出维度不匹配
- ⚠️ 激活函数选错

**4. 超参数选择**
- 隐藏层大小：太小欠拟合，太大过拟合
- 学习率：太大震荡，太小收敛慢
- epoch 数：太少没学会，太多过拟合

**5. PyTorch 核心类**
- `nn.Module`：网络基类
- `nn.Linear`：全连接层
- `nn.BCELoss`：二元交叉熵
- `optim.Adam`：自适应优化器

---

## 课后练习

给大家留几道练习题：

1. **【简单】** 把隐藏层神经元从 8 改成 16，观察效果
2. **【中等】** 在 fc1 和 fc2 之间再加一层，记得也要加激活函数
3. **【中等】** 把 ReLU 换成 Tanh 或 LeakyReLU，对比效果
4. **【进阶】** 修改下雨规则，让它更复杂，看神经网络能不能学会
5. **【进阶】** 分离训练集和测试集，80% 训练，20% 测试

---

## 下节预告

今天我们学了最简单的**前馈神经网络**。

下节课，我们来学**卷积神经网络（CNN）**，它特别擅长处理图像。

我们会用 CNN 来识别手写数字，非常有趣！

好，今天就到这里，大家辛苦了！有问题随时问我。

---

*讲稿完*
