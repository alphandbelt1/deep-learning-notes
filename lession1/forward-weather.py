"""
╔══════════════════════════════════════════════════════════════════════════════╗
║                    🌧️ 神经网络入门：天气预测 🌧️                         ║
╠══════════════════════════════════════════════════════════════════════════════╣
║  🎯 学习目标：                                                                ║
║     1. 理解什么是前馈神经网络（Feedforward Neural Network）                    ║
║     2. 掌握 PyTorch 搭建神经网络的基本流程                                     ║
║     3. 理解训练过程：前向传播 → 计算损失 → 反向传播 → 更新权重                  ║
║                                                                              ║
║  🧠 生活比喻：                                                                ║
║     神经网络就像一个"学徒厨师"：                                              ║
║     - 输入数据 = 食材（云量、风速、湿度）                                      ║
║     - 网络权重 = 厨师的烹饪经验                                               ║
║     - 输出结果 = 做出的菜品（下雨/不下雨的预测）                               ║
║     - 训练过程 = 师傅不断纠正学徒，学徒慢慢积累经验                            ║
║                                                                              ║
║  📊 数据流程图：                                                              ║
║                                                                              ║
║     [云量]  ─┐                                                               ║
║              │     ┌──────────┐     ┌──────────┐     ┌──────────┐           ║
║     [风速]  ─┼────►│ 隐藏层   │────►│ 输出层   │────►│ 0~1概率  │           ║
║              │     │ (8神经元) │     │ (1神经元) │     │ 下雨概率  │           ║
║     [湿度]  ─┘     └──────────┘     └──────────┘     └──────────┘           ║
║                         ↑                ↑                                   ║
║                      ReLU激活        Sigmoid激活                             ║
╚══════════════════════════════════════════════════════════════════════════════╝
"""

# ============================================================================
# 📦 第一步：导入必要的库
# ============================================================================
#
# ═══════════════════════════════════════════════════════════════════════════════
# 【深度学习工具链的模块化设计哲学】
# ═══════════════════════════════════════════════════════════════════════════════
#
# 现代深度学习框架遵循「关注点分离」(Separation of Concerns) 原则：
# 每个模块负责一个独立的抽象层次，便于组合、测试和维护。
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │  模块层次结构                                                                │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │                                                                             │
# │   numpy          →  数值计算基础层（N维数组、线性代数、随机数）              │
# │      ↓                                                                      │
# │   torch          →  张量计算引擎（GPU加速 + 自动微分）                      │
# │      ↓                                                                      │
# │   torch.nn       →  神经网络抽象层（层、损失函数、容器）                    │
# │      ↓                                                                      │
# │   torch.optim    →  优化算法层（SGD、Adam、学习率调度）                     │
# │                                                                             │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ═══════════════════════════════════════════════════════════════════════════════

import torch
#
# ═══════════════════════════════════════════════════════════════════════════════
# 【torch 模块详解】
# ═══════════════════════════════════════════════════════════════════════════════
#
# torch 是 PyTorch 的核心命名空间，提供：
#
# 1. Tensor 类：多维数组的核心数据结构
#    - 数学上：Tensor 是向量空间的多线性映射（但在 ML 中简化为"多维数组"）
#    - 工程上：封装了 CPU/GPU 内存管理、类型转换、形状操作
#    - 关键属性：
#      · .data      → 底层数据存储
#      · .grad      → 梯度存储（反向传播后填充）
#      · .grad_fn   → 计算图中的父节点（用于自动微分）
#      · .device    → 存储位置（cpu / cuda:0 / mps）
#      · .dtype     → 数据类型（float32 / float64 / int64 等）
#
# 2. 自动微分引擎 (Autograd)
#    - 基于「计算图」(Computational Graph) 的反向模式自动微分
#    - 时间复杂度：O(1) 相对于参数数量（反向传播的核心优势）
#    - 支持高阶导数（Hessian、Jacobian）
#
# 3. 设备抽象
#    - torch.cuda.* → NVIDIA GPU
#    - torch.mps.*  → Apple Silicon GPU (M1/M2/M3)
#    - torch.xpu.*  → Intel GPU
#
# ═══════════════════════════════════════════════════════════════════════════════

import torch.nn as nn
#
# ═══════════════════════════════════════════════════════════════════════════════
# 【torch.nn 模块详解】
# ═══════════════════════════════════════════════════════════════════════════════
#
# nn 模块是神经网络的「乐高积木盒」，提供：
#
# 1. 基类 nn.Module
#    - 所有神经网络层的父类
#    - 核心方法：
#      · __init__()  → 定义层结构（声明式）
#      · forward()   → 定义前向传播（命令式）
#      · parameters() → 迭代所有可学习参数
#      · to(device)  → 将整个网络移动到指定设备
#
# 2. 常用层（Layer）
#    - nn.Linear(in, out)      → 全连接层：y = xW^T + b
#    - nn.Conv2d(in, out, k)   → 2D卷积层（图像处理）
#    - nn.LSTM(in, hidden)     → 长短期记忆网络（序列建模）
#    - nn.Transformer(...)     → Transformer（NLP/CV通用）
#
# 3. 激活函数（Activation）
#    - nn.ReLU()     → max(0, x)，隐藏层首选
#    - nn.Sigmoid()  → 1/(1+e^{-x})，二分类输出层
#    - nn.Softmax()  → e^{x_i}/Σe^{x_j}，多分类输出层
#    - nn.GELU()     → Gaussian Error Linear Unit（Transformer标配）
#
# 4. 损失函数（Loss）
#    - nn.MSELoss()            → 均方误差（回归）
#    - nn.CrossEntropyLoss()   → 交叉熵（多分类，内含Softmax）
#    - nn.BCELoss()            → 二元交叉熵（二分类）
#
# ═══════════════════════════════════════════════════════════════════════════════

import torch.optim as optim
#
# ═══════════════════════════════════════════════════════════════════════════════
# 【torch.optim 模块详解】
# ═══════════════════════════════════════════════════════════════════════════════
#
# optim 模块封装了各种优化算法，核心职责：
#   「根据梯度信息，更新模型参数，使损失函数最小化」
#
# 1. 基类 optim.Optimizer
#    - 核心方法：
#      · zero_grad()  → 清空所有参数的梯度
#      · step()       → 执行一步参数更新
#      · state_dict() → 保存优化器状态（用于断点续训）
#
# 2. 经典优化器演进路线
#
#    SGD (1951)                    → 最基础，更新规则：θ ← θ - η∇L
#      ↓
#    Momentum (1964)               → 加入动量，平滑震荡
#      ↓
#    AdaGrad (2011)                → 自适应学习率（稀疏特征友好）
#      ↓
#    RMSprop (2012, Hinton)        → 修复 AdaGrad 学习率衰减过快
#      ↓
#    Adam (2014, Kingma & Ba)      → 结合 Momentum + RMSprop，工业首选
#      ↓
#    AdamW (2017)                  → 修正 Adam 的权重衰减实现
#      ↓
#    LAMB/LARS (2019)              → 大批量训练专用
#
# 3. 学习率调度器 (optim.lr_scheduler)
#    - StepLR        → 每隔 N 步衰减
#    - CosineAnnealingLR → 余弦退火
#    - OneCycleLR    → 1Cycle 策略（快速收敛）
#
# ═══════════════════════════════════════════════════════════════════════════════

import numpy as np
#
# ═══════════════════════════════════════════════════════════════════════════════
# 【numpy 模块详解】
# ═══════════════════════════════════════════════════════════════════════════════
#
# NumPy (Numerical Python) 是科学计算的基石，提供：
#
# 1. ndarray：高效的 N 维数组
#    - 内存连续存储（C-order / Fortran-order）
#    - 支持广播 (Broadcasting)：不同形状数组的隐式扩展
#    - 支持切片 (Slicing)：零拷贝视图操作
#
# 2. 向量化运算 (Vectorization)
#    - 底层用 C/Fortran 实现，利用 SIMD 指令集（SSE/AVX）
#    - 比 Python 原生循环快 10~100 倍
#
# 3. 随机数生成 (numpy.random)
#    - 使用 Mersenne Twister 伪随机数生成器（周期 2^19937-1）
#    - 支持多种分布：均匀、正态、泊松、伯努利等
#
# 4. 与 PyTorch 的互操作
#    - torch.from_numpy(arr)  → NumPy → Tensor（共享内存！）
#    - tensor.numpy()         → Tensor → NumPy（CPU only）
#
# ═══════════════════════════════════════════════════════════════════════════════
# 【NumPy ↔ PyTorch 互操作的深度解析】
# ═══════════════════════════════════════════════════════════════════════════════
#
# 一、为什么需要互操作？
#
#    实际项目中，数据处理链路常常是：
#
#        原始数据 → NumPy 预处理 → PyTorch 训练 → NumPy 后处理 → 保存/可视化
#
#    例如：
#    - 用 NumPy 读取 CSV、处理图像、做统计
#    - 用 PyTorch 构建模型、GPU 训练
#    - 用 NumPy + Matplotlib 绘图
#
# 二、两种转换方式的内存行为
#
#    ┌─────────────────────────────────────────────────────────────────────────┐
#    │  方式 1：torch.from_numpy(arr) —— 共享内存（零拷贝）                      │
#    ├─────────────────────────────────────────────────────────────────────────┤
#    │                                                                         │
#    │    NumPy ndarray                    PyTorch Tensor                      │
#    │    ┌─────────────┐                  ┌─────────────┐                     │
#    │    │ 元数据      │                  │ 元数据      │                     │
#    │    │ shape, dtype│                  │ shape, dtype│                     │
#    │    └──────┬──────┘                  └──────┬──────┘                     │
#    │           │                                │                            │
#    │           └────────────┬───────────────────┘                            │
#    │                        ▼                                                │
#    │              ┌─────────────────┐                                        │
#    │              │ 实际数据存储     │  ← 同一块内存！                         │
#    │              │ [1.0, 2.0, 3.0] │                                        │
#    │              └─────────────────┘                                        │
#    │                                                                         │
#    │    优点：速度快（O(1)），不占用额外内存                                    │
#    │    缺点：修改一方会影响另一方！                                           │
#    │                                                                         │
#    └─────────────────────────────────────────────────────────────────────────┘
#
#    ┌─────────────────────────────────────────────────────────────────────────┐
#    │  方式 2：torch.tensor(arr) —— 深拷贝（独立内存）                          │
#    ├─────────────────────────────────────────────────────────────────────────┤
#    │                                                                         │
#    │    NumPy ndarray                    PyTorch Tensor                      │
#    │    ┌─────────────┐                  ┌─────────────┐                     │
#    │    │ 元数据      │                  │ 元数据      │                     │
#    │    └──────┬──────┘                  └──────┬──────┘                     │
#    │           │                                │                            │
#    │           ▼                                ▼                            │
#    │    ┌─────────────┐                  ┌─────────────┐                     │
#    │    │ 数据副本 A  │                  │ 数据副本 B  │  ← 各自独立！         │
#    │    │ [1.0,2.0,..]│                  │ [1.0,2.0,..]│                     │
#    │    └─────────────┘                  └─────────────┘                     │
#    │                                                                         │
#    │    优点：互不干扰，安全                                                   │
#    │    缺点：需要复制数据（O(n)），占用双倍内存                                │
#    │                                                                         │
#    └─────────────────────────────────────────────────────────────────────────┘
#
# 三、代码示例：共享内存的"陷阱"
#
#    import numpy as np
#    import torch
#
#    # 创建 NumPy 数组
#    arr = np.array([1.0, 2.0, 3.0])
#
#    # 方式 1：共享内存
#    tensor_shared = torch.from_numpy(arr)
#
#    # 修改 NumPy 数组
#    arr[0] = 999.0
#
#    print(tensor_shared)  # tensor([999., 2., 3.]) ← NumPy 的修改影响了 Tensor！
#
#    # 方式 2：独立副本
#    tensor_copy = torch.tensor(arr)  # 或 torch.from_numpy(arr).clone()
#
#    arr[0] = 111.0
#    print(tensor_copy)    # tensor([999., 2., 3.]) ← 不受影响
#
# 四、反向转换：Tensor → NumPy
#
#    tensor.numpy() 的注意事项：
#
#    1. 仅支持 CPU Tensor：
#       gpu_tensor.numpy()      # ❌ 报错！
#       gpu_tensor.cpu().numpy()  # ✓ 先移回 CPU
#
#    2. 默认也是共享内存：
#       t = torch.tensor([1.0, 2.0])
#       a = t.numpy()
#       t[0] = 999.0
#       print(a)  # [999., 2.] ← 被影响了！
#
#    3. 需要独立副本时：
#       a = t.clone().numpy()        # 方式 1：先克隆
#       a = t.detach().numpy().copy()  # 方式 2：用 NumPy 的 copy
#
# 五、requires_grad=True 时的特殊处理
#
#    如果 Tensor 需要梯度，不能直接转 NumPy（会破坏计算图）：
#
#    t = torch.tensor([1.0], requires_grad=True)
#    t.numpy()  # ❌ RuntimeError: Can't call numpy() on Tensor that requires grad
#
#    解决方案：
#    t.detach().numpy()  # ✓ detach() 创建一个不需要梯度的视图
#
# 六、最佳实践总结
#
#    场景                               推荐方式
#    ─────────────────────────────────────────────────────────────
#    大数据、只读、追求性能              torch.from_numpy(arr)
#    需要独立修改、避免 bug              torch.tensor(arr)
#    训练后的预测结果转 NumPy            output.detach().cpu().numpy()
#    需要完全独立的副本                  tensor.clone().detach().numpy()
#
# ═══════════════════════════════════════════════════════════════════════════════
#
# ═══════════════════════════════════════════════════════════════════════════════

# ============================================================================
# 注释风格与约定（本文件统一遵循，便于教学与维护）
# ============================================================================
# 1) 语言：中文注释，面向初学者与工程实践结合
# 2) 注释粒度：每个逻辑块、每个重要变量、每个数组变换、每次维度/类型变化
# 3) 数据来源说明：明确注明数据是如何生成/来源于文件/或外部采集
# 4) 概念解释：遇到重要概念（广播、向量化、计算图、激活函数等）均给出简短定义
# 5) 形式化示例：用小规模数值示例说明逐元素运算和矩阵形状变化
# 6) 风格一致性：使用『╔═』样式的分隔块标识重要解释区
#
# 说明：接下来我将分块重写/统一本文件的注释，保证每段代码附近都有清晰的
#       数据去向（来源/变换/去处）和遇到概念的直观解释。如需我一次性完成整
#       个文件的重写，请回复确认“现在开始全部重写”，否则我将按模块逐步提交。

print("=" * 60)
print("🌧️  神经网络天气预测 - 学习前馈网络的第一步")
print("=" * 60)

# ============================================================================
# 🎲 第二步：设置随机种子（确保实验可复现）
# ============================================================================
#
# ═══════════════════════════════════════════════════════════════════════════════
# 【随机性与可复现性：科学实验的基石】
# ═══════════════════════════════════════════════════════════════════════════════
#
# 一、为什么深度学习中存在随机性？
#
#    随机性来源：
#    1. 权重初始化：Xavier/He/Kaiming 初始化从特定分布采样
#    2. 数据加载：DataLoader 的 shuffle=True 打乱顺序
#    3. Dropout：训练时随机丢弃神经元（正则化）
#    4. 数据增强：随机裁剪、翻转、色彩抖动
#    5. 批归一化：running_mean/running_var 依赖批次顺序
#
# 二、伪随机数生成器 (PRNG) 的数学原理
#
#    计算机无法生成"真随机数"，而是用确定性算法生成"伪随机数"。
#
#    Mersenne Twister 算法（NumPy 和 PyTorch 默认）：
#    - 周期：2^19937 - 1（极长，几乎不会循环）
#    - 均匀分布：通过 623 维等分布保证统计质量
#    - 状态向量：624 个 32 位整数
#
#    核心公式（线性反馈移位寄存器）：
#        x_{n+624} = x_{n+397} ⊕ (x_n^{upper} | x_{n+1}^{lower}) A
#
#    其中：
#        ⊕ = 异或运算
#        A = 固定的矩阵变换
#        upper/lower = 高位/低位掩码
#
# 三、种子 (Seed) 的作用
#
#    种子是 PRNG 的初始状态。相同的种子 → 相同的随机数序列。
#
#    数学表示：
#        f: Seed → State_0 → State_1 → State_2 → ...
#                    ↓          ↓          ↓
#                  rand_0     rand_1     rand_2
#
#    示例（简化）：
#        seed=42  →  [r₀=0.374, r₁=0.950, r₂=0.731, ...]
#        seed=42  →  [r₀=0.374, r₁=0.950, r₂=0.731, ...]  ← 完全相同！
#        seed=123 →  [r₀=0.696, r₁=0.286, r₂=0.226, ...]  ← 完全不同
#
# 四、可复现性的工程实践
#
#    学术论文要求：
#    - 必须公开随机种子
#    - 必须固定所有随机源
#    - 理想情况下，他人运行代码应得到相同结果
#
#    工业部署要求：
#    - 训练时：固定种子便于调试
#    - 推理时：通常关闭随机性（eval mode）
#
# 五、完整的可复现性配置（生产级）
#
#    import random
#    import os
#    SEED = 42
#    random.seed(SEED)                          # Python 内置随机
#    np.random.seed(SEED)                       # NumPy 随机
#    torch.manual_seed(SEED)                    # PyTorch CPU 随机
#    torch.cuda.manual_seed(SEED)               # PyTorch 单 GPU 随机
#    torch.cuda.manual_seed_all(SEED)           # PyTorch 多 GPU 随机
#    torch.backends.cudnn.deterministic = True  # cuDNN 确定性模式
#    torch.backends.cudnn.benchmark = False     # 禁用自动调优
#    os.environ['PYTHONHASHSEED'] = str(SEED)   # Python hash 种子
#
# ═══════════════════════════════════════════════════════════════════════════════

torch.manual_seed(42)
#
# torch.manual_seed(seed) 做了什么？
#    1. 初始化 PyTorch 的默认随机数生成器
#    2. 影响所有 torch.rand*、torch.randn*、nn.init.* 等操作
#    3. 不影响 NumPy 或 Python 内置 random 模块
#
# 返回值：torch.Generator 对象（可用于更细粒度的控制）

np.random.seed(42)
#
# np.random.seed(seed) 做了什么？
#    1. 初始化 NumPy 的全局随机数生成器
#    2. 影响所有 np.random.* 函数
#    3. 不影响 PyTorch 或 Python 内置 random 模块
#
# 💡 42 是个"梗"，来自道格拉斯·亚当斯的《银河系漫游指南》：
#    "生命、宇宙以及一切的终极答案是……42。"
#    在机器学习社区中，42 几乎成了随机种子的「默认值」。

# ============================================================================
# 📊 第三步：生成模拟天气数据
# ============================================================================
# 🧠 生活比喻：我们在"造假"天气记录
#    真实世界：气象站收集了几十年的数据
#    我们这里：用规则模拟，快速理解原理
#
# 💡 核心逻辑：
#    1. 上帝视角（我们）：知道下雨的真实规则（湿度+云量 > 风速*1.2）
#    2. 凡人视角（模型）：只看到数据（云、风、湿 -> 下雨/不下雨），不知道规则
#    3. 目标：让模型通过观察数据，自己"猜"出这个规则

print("\n📊 [第1步] 生成训练数据...")

num_samples = 500  # 生成 500 条"假"天气记录
                   # 🔑 样本数量的选择：太少学不好，太多训练慢
                   # 💡 500 是入门练习的合适数量

# ----------------------------------------------------------------------------
# 生成三个输入特征（0~100 的整数）
# ----------------------------------------------------------------------------
# 想象成气象站的三个传感器读数：

cloud = np.random.randint(0, 100, num_samples)     # 云层厚度：0=晴空万里，100=乌云密布
wind = np.random.randint(0, 100, num_samples)      # 风力大小：0=无风，100=狂风
humidity = np.random.randint(0, 100, num_samples)  # 空气湿度：0=干燥，100=潮湿
#
# ═══════════════════════════════════════════════════════════════════════════
# 【np.random.randint 函数详解】
# ═══════════════════════════════════════════════════════════════════════════
#
# 函数签名：np.random.randint(low, high=None, size=None, dtype=int)
#
# 一、参数解析
#
#    np.random.randint(0, 100, num_samples)
#                      │   │      │
#                      │   │      └─► size=500，生成 500 个随机数
#                      │   └────────► high=100，上界（不包含）
#                      └────────────► low=0，下界（包含）
#
#    生成范围：[0, 100)，即 0, 1, 2, ..., 99 中随机选取
#
# 二、返回值的形状和类型
#
#    cloud.shape = (500,)      # 一维数组，500 个元素
#    cloud.dtype = int64       # 默认是 64 位整数
#
#    具体内容（示例）：
#        cloud = [51, 92, 14, 71, 60, 20, ...]   # 500 个 0~99 的随机整数
#
# 三、随机种子的影响
#
#    因为前面设置了 np.random.seed(42)，所以每次运行结果相同：
#
#        第 1 次运行：cloud = [51, 92, 14, 71, 60, ...]
#        第 2 次运行：cloud = [51, 92, 14, 71, 60, ...]  ← 完全一致！
#
#    如果不设置种子：
#        第 1 次运行：cloud = [51, 92, 14, 71, 60, ...]
#        第 2 次运行：cloud = [38, 7, 85, 23, 99, ...]   ← 每次不同
#
# 四、为什么用整数而不是浮点数？
#
#    1. 模拟真实传感器：气象站读数通常是整数（如湿度 75%）
#    2. 简化问题：整数更直观，便于理解
#    3. 后续会转成 float32：不影响训练
#
# 五、三个数组的独立性
#
#    cloud、wind、humidity 是【独立生成】的：
#    - 它们之间没有相关性
#    - 每个数组都从相同的均匀分布中采样
#    - 这模拟了真实世界中气象变量的随机性
#
#    数据示意：
#        样本 i     cloud[i]    wind[i]    humidity[i]
#        ─────────────────────────────────────────────
#        0          51          92         71
#        1          14          71         60
#        2          60          20         82
#        ...        ...         ...        ...
#        499        38          55         47
#
# ═══════════════════════════════════════════════════════════════════════════

# ----------------------------------------------------------------------------
# 生成标签（是否下雨）—— 这是我们"编造"的下雨规则
# ----------------------------------------------------------------------------
# 🔑 规则：湿度 + 云量 > 风速 × 1.2 时，下雨
#
# 🧠 生活直觉：
#    - 湿度高 + 云多 → 容易下雨 ✅
#    - 风大 → 把云吹散，不容易下雨 ✅
#    - 1.2 是个"经验系数"，让规则更有区分度
#
# ⚠️ 注意：真实天气预测比这复杂得多！这只是教学示例

labels = ((humidity + cloud) > wind * 1.2).astype(np.float32)
#
# ═══════════════════════════════════════════════════════════════════════════
# 【NumPy 向量化运算的逐步拆解】
# ═══════════════════════════════════════════════════════════════════════════
#
# 这一行代码包含了 NumPy 最核心的特性：向量化运算 (Vectorized Operations)。
# 让我们逐步拆解，假设只有 5 个样本：
#
#    humidity = [45, 12, 95, 30, 67]   # 5 个湿度值
#    cloud    = [62, 27, 53, 88, 41]   # 5 个云量值
#    wind     = [39, 81,  7, 23, 55]   # 5 个风速值
#
# 一、加法运算：humidity + cloud
#
#    NumPy 的加法是【逐元素】(element-wise) 进行的：
#
#        humidity = [45, 12, 95, 30, 67]
#                    +   +   +   +   +      ← 对应位置相加
#        cloud    = [62, 27, 53, 88, 41]
#                    ↓   ↓   ↓   ↓   ↓
#        结果      = [107, 39, 148, 118, 108]
#
#    等价于 Python 循环（但 NumPy 底层用 C 实现，快 100 倍）：
#        result = [humidity[i] + cloud[i] for i in range(5)]
#
# 二、乘法运算：wind * 1.2
#
#    这是数组与标量的运算，NumPy 会【广播】(broadcast) 标量：
#
#        wind = [39,   81,   7,    23,   55  ]
#                ×     ×     ×     ×     ×      ← 每个元素都乘以 1.2
#               1.2   1.2   1.2   1.2   1.2
#                ↓     ↓     ↓     ↓     ↓
#        结果  = [46.8, 97.2, 8.4, 27.6, 66.0]
#
#    广播的直觉：把标量 1.2 "复制"成 [1.2, 1.2, 1.2, 1.2, 1.2]，
#              然后做逐元素乘法。
#
# 三、比较运算：(humidity + cloud) > (wind * 1.2)
#
#    比较运算也是【逐元素】进行，返回布尔数组：
#
#        左边 = [107,  39,   148,  118,  108 ]
#                >     >     >     >     >       ← 逐元素比较
#        右边 = [46.8, 97.2, 8.4,  27.6, 66.0]
#                ↓     ↓     ↓     ↓     ↓
#        结果  = [True, False, True, True, True]
#
#        解读：
#        - 样本 0: 107 > 46.8  → True  (下雨)
#        - 样本 1:  39 > 97.2  → False (不下雨)
#        - 样本 2: 148 > 8.4   → True  (下雨)
#        - 样本 3: 118 > 27.6  → True  (下雨)
#        - 样本 4: 108 > 66.0  → True  (下雨)
#
# 四、类型转换：.astype(np.float32)
#
#        布尔数组  = [True,  False, True, True, True]
#                      ↓      ↓      ↓     ↓     ↓
#        float32   = [1.0,   0.0,   1.0,  1.0,  1.0]
#
#        True → 1.0，False → 0.0
#
# 五、完整流程图
#
#    humidity  cloud   wind
#       │        │       │
#       │        │       │×1.2
#       │        │       ▼
#       │        │    wind*1.2
#       └───+────┘       │
#           │            │
#           ▼            │
#     humidity+cloud     │
#           │            │
#           └─────>──────┘
#                 │
#                 ▼
#          逐元素比较 (>)
#                 │
#                 ▼
#         [True, False, ...]
#                 │
#                 ▼ .astype(np.float32)
#         [1.0, 0.0, ...]
#
# 六、为什么叫"向量化"？
#
#    传统编程思维：用 for 循环逐个处理
#        for i in range(500):
#            if humidity[i] + cloud[i] > wind[i] * 1.2:
#                labels[i] = 1.0
#            else:
#                labels[i] = 0.0
#
#    向量化思维：把整个数组当作一个整体操作
#        labels = ((humidity + cloud) > wind * 1.2).astype(np.float32)
#
#    向量化的优势：
#    - 代码简洁（1 行 vs 5 行）
#    - 速度快（C 实现 + SIMD 指令 + 缓存优化）
#    - 更符合数学表达（像写公式一样写代码）
#
# ═══════════════════════════════════════════════════════════════════════════
#
#         ↑ 1. 布尔表达式：结果是 [True, False, True...]
#                                         ↑ 2. 为什么要转成 float32？(详细解答)
#                                              A. 为什么不是 int？
#                                                 - 损失函数 BCELoss 内部涉及 log 计算，需要连续的浮点数空间。
#                                                 - 即使是 0/1 标签，数学上也被视为概率分布的特例(1.0/0.0)。
#                                              B. 为什么不是 float64 (double)？
#                                                 - NumPy 默认是 float64，精度极高但占 8 字节。
#                                                 - 深度学习通常不需要那么高精度，float32 (4 字节) 足够了。
#                                                 - 显卡(GPU) 跑 float32 比 float64 快很多（几倍到几十倍）。
#                                                 - 内存/显存占用减半，能训练更大的模型。
#                                              C. 还有其他选择吗？(枚举)
#                                                 - float16 (半精度): 显存极度紧张时用，容易溢出。
#                                                 - bfloat16: 新型 GPU 专用，兼顾范围和速度。
#                                                 - int8: 模型量化部署到手机时用。

# 让我们看看数据长什么样
print(f"   云量前5个: {cloud[:5]}")
print(f"   风速前5个: {wind[:5]}")
print(f"   湿度前5个: {humidity[:5]}")
print(f"   标签前5个: {labels[:5]}  (1=下雨, 0=不下雨)")
print(f"   下雨样本占比: {labels.mean()*100:.1f}%")
#                                 ↑ 1. .mean(): 计算平均值。因为标签是 0 和 1，平均值就是 1 的占比。
#                                      例如: [0, 1, 1, 0] -> sum=2, count=4 -> mean=0.5 (50%是1)
#                                 ↑ 2. *100: 变成百分数 (0.74 -> 74.0)
#                                 ↑ 3. :.1f: 保留一位小数

# ----------------------------------------------------------------------------
# 组装数据并转换为 PyTorch 张量（Tensor）
# ----------------------------------------------------------------------------
# 🔑 Tensor 是什么？
#    - NumPy 数组的"升级版"
#    - 可以在 GPU 上运算（快！）
#    - 支持自动求导（训练神经网络的关键）

# 把三个特征"堆叠"成一个矩阵
#
# ═══════════════════════════════════════════════════════════════════════════
# 【特征矩阵的构造与数据类型选择】
# ═══════════════════════════════════════════════════════════════════════════
#
# 一、为什么要构造特征矩阵？
#
#    神经网络的输入必须是一个统一的数学对象。
#    
#    我们有三个独立的特征向量：
#    - cloud:    [c₁, c₂, ..., c₅₀₀]
#    - wind:     [w₁, w₂, ..., w₅₀₀]  
#    - humidity: [h₁, h₂, ..., h₅₀₀]
#
#    需要将它们组织成【设计矩阵】(Design Matrix) X：
#
#        X = | c₁  w₁  h₁ |      每一行是一个样本
#            | c₂  w₂  h₂ |      每一列是一个特征
#            | ...        |
#            | c₅₀₀ w₅₀₀ h₅₀₀ |
#
#    这是机器学习的标准数据格式：X ∈ ℝ^(n×d)，n 个样本，d 个特征。
#
# 二、np.stack 的几何意义
#
#    stack 沿新轴堆叠数组。axis=1 表示沿第二个维度（列方向）堆叠。
#
#    原始：3 个形状为 (500,) 的向量
#    结果：1 个形状为 (500, 3) 的矩阵
#
#    这个操作在线性代数中等价于将三个列向量并排放置。
#
# 三、为什么必须转换为 float32？
#
#    这涉及计算机的数值表示和深度学习的工程实践。
#
#    1. 精度与存储的权衡
#       - float64 (双精度): 64 位，约 15-17 位有效数字
#       - float32 (单精度): 32 位，约 6-9 位有效数字
#
#       神经网络的权重更新本质上是近似计算，6-9 位精度完全足够。
#       使用 float32 可以：
#       - 内存占用减半
#       - GPU 吞吐量翻倍（单精度运算单元更多）
#       - 显存容量翻倍（可训练更大的模型）
#
#    2. 类型系统的严格性
#       PyTorch 的矩阵运算要求操作数类型完全一致。
#       nn.Linear 的权重默认初始化为 float32。
#       若输入为 float64，则 X @ W 会触发类型错误。
#
#    3. 工业界的共识
#       自 2012 年 AlexNet 以来，float32 成为深度学习的事实标准。
#       近年来甚至出现 float16/bfloat16 的混合精度训练。
#
X = np.stack([cloud, wind, humidity], axis=1).astype(np.float32)

# ═══════════════════════════════════════════════════════════════════════════
# 【标签向量的形状变换】
# ═══════════════════════════════════════════════════════════════════════════
#
# 原始 labels 的形状是 (500,)，这是一个【秩为 1 的张量】（向量）。
# 神经网络输出的形状是 (500, 1)，这是一个【秩为 2 的张量】（列向量/矩阵）。
#
# 为什么要区分 (500,) 和 (500, 1)？
#
#    在数学上，它们都表示 500 个数。
#    但在张量运算中，形状不匹配会导致广播 (broadcasting) 行为不可预测。
#
#    例如：
#    - (500, 1) - (500,) 会触发广播，可能产生 (500, 500) 的结果
#    - (500, 1) - (500, 1) 则是逐元素相减，结果仍是 (500, 1)
#
#    为避免歧义，标签应与网络输出保持相同的形状。
#
# reshape(-1, 1) 的含义：
#    -1 是占位符，表示"根据总元素数自动推断"。
#    这里 500 个元素，重塑为 (?, 1)，则 ? = 500。
#
y = labels.reshape(-1, 1)

# ═══════════════════════════════════════════════════════════════════════════
# 【X 和 y 最终是什么？—— 完整的数据格式总结】
# ═══════════════════════════════════════════════════════════════════════════
#
# 一、X 的最终形态
#
#    X 是一个 500 × 3 的二维数组（矩阵），数据类型为 float32。
#
#    具体结构：
#
#        X = | 62.0  39.0  45.0 |    ← 第 0 个样本：云量=62, 风速=39, 湿度=45
#            | 27.0  81.0  12.0 |    ← 第 1 个样本：云量=27, 风速=81, 湿度=12
#            | 53.0  7.0   95.0 |    ← 第 2 个样本：云量=53, 风速=7,  湿度=95
#            |  ...   ...   ... |
#            | 88.0  23.0  67.0 |    ← 第 499 个样本
#
#    访问方式：
#    - X[0]     → [62.0, 39.0, 45.0]   # 第 0 个样本的所有特征
#    - X[:, 0]  → [62.0, 27.0, 53.0, ...]  # 所有样本的云量
#    - X[2, 1]  → 7.0   # 第 2 个样本的风速
#
# 二、y 的最终形态
#
#    y 是一个 500 × 1 的二维数组（列向量），数据类型为 float32。
#
#    具体结构：
#
#        y = | 1.0 |    ← 第 0 个样本的标签：下雨
#            | 0.0 |    ← 第 1 个样本的标签：不下雨
#            | 1.0 |    ← 第 2 个样本的标签：下雨
#            | ... |
#            | 1.0 |    ← 第 499 个样本的标签
#
#    每个值只有两种可能：
#    - 1.0 表示"下雨"
#    - 0.0 表示"不下雨"
#
# 三、为什么 X 是 (n, d) 而 y 是 (n, 1)？
#
#    这是机器学习的标准数据格式，源于线性代数的矩阵乘法：
#
#    神经网络的计算本质是：
#
#        ŷ = σ(X @ W + b)
#
#    其中：
#    - X ∈ ℝ^(n×d)：n 个样本，每个样本 d 个特征
#    - W ∈ ℝ^(d×1)：权重矩阵（第一层简化版）
#    - b ∈ ℝ^(1×1)：偏置
#    - ŷ ∈ ℝ^(n×1)：预测值（n 个样本的预测）
#    - σ：激活函数
#
#    矩阵乘法 X @ W 要求：
#    - X 的列数 = W 的行数（即 d = d）
#    - 结果形状 = (n, 1)
#
#    因此，y 必须是 (n, 1) 才能与 ŷ 进行逐元素比较计算损失。
#
# 四、为什么不直接用 (500,) 的一维向量？
#
#    技术原因：PyTorch 的损失函数和网络输出都是二维的。
#
#    神经网络输出 shape = (batch_size, output_dim) = (500, 1)
#    若 y 是 (500,)，则：
#
#        loss = criterion(output, y)
#        # output.shape = (500, 1)
#        # y.shape = (500,)
#        # → 触发 broadcasting：(500, 1) vs (500,) → (500, 500)
#        # → 计算出错误的损失值！
#
#    将 y 变成 (500, 1) 后：
#
#        # output.shape = (500, 1)
#        # y.shape = (500, 1)
#        # → 逐元素对应，计算正确
#
# 五、数据流的完整视角
#
#    原始数据        NumPy 处理           PyTorch 训练
#    ─────────────────────────────────────────────────────
#    cloud (500,)  ─┐
#    wind (500,)   ─┼─► X (500, 3) ──► X_tensor ──► model ──► output (500, 1)
#    humidity (500,)┘                                              ↓
#                                                              BCELoss
#    labels (500,) ──► y (500, 1) ──► y_tensor ───────────────────►↑
#
#    整个流程确保了维度的一致性，这是深度学习工程中最常见的 bug 来源之一。
#
# ═══════════════════════════════════════════════════════════════════════════

# 从 NumPy 转换为 PyTorch Tensor
#
# ═══════════════════════════════════════════════════════════════════════════
# 【Tensor：深度学习的计算基元】
# ═══════════════════════════════════════════════════════════════════════════
#
# 一、什么是 Tensor？
#
#    在数学中，Tensor（张量）是向量和矩阵的推广。
#    - 标量是 0 阶张量（一个数）
#    - 向量是 1 阶张量（一列数）
#    - 矩阵是 2 阶张量（一张表）
#    - 更高维的数据结构是 n 阶张量
#
#    在深度学习框架中，Tensor 不仅仅是"多维数组"。
#    它是一个携带【计算历史】的数据容器。
#
# 二、NumPy Array vs PyTorch Tensor 的本质区别
#
#    NumPy Array:
#    - 纯粹的数值容器
#    - 执行 c = a + b 后，c 只知道自己的值，不知道自己来自 a 和 b
#
#    PyTorch Tensor:
#    - 数值容器 + 计算图节点
#    - 执行 c = a + b 后，c 内部记录："我是由 a 和 b 通过加法产生的"
#    - 这个记录存储在 c.grad_fn 属性中
#
# 三、为什么这种区别至关重要？
#
#    深度学习的核心是【梯度下降优化】：
#
#        θ_new = θ_old - η × ∂L/∂θ
#
#    其中 ∂L/∂θ 是损失函数对参数的偏导数。
#
#    对于复杂的神经网络，L 是由数十层运算复合而成的函数。
#    手动推导 ∂L/∂θ 是不可行的。
#
#    解决方案：【自动微分】(Automatic Differentiation)
#
#    自动微分的原理：
#    - 前向传播时，记录每一步运算（构建计算图）
#    - 反向传播时，沿计算图逆向应用链式法则
#
#    链式法则：若 L = f(g(h(θ)))，则
#        ∂L/∂θ = (∂f/∂g) × (∂g/∂h) × (∂h/∂θ)
#
#    PyTorch 在调用 loss.backward() 时，自动完成上述计算。
#    但前提是：所有中间结果都必须是 Tensor，否则计算图断裂。
#
# 四、总结
#
#    NumPy：为数值计算而生，不追踪计算历史
#    Tensor：为可微分计算而生，是自动微分引擎的载体
#
#    转换为 Tensor，本质上是将数据接入 PyTorch 的自动微分系统。
#
# ═══════════════════════════════════════════════════════════════════════════
#
X_tensor = torch.from_numpy(X)  # 训练输入
y_tensor = torch.from_numpy(y)  # 训练标签（正确答案）

print(f"\n   X_tensor 形状: {X_tensor.shape}  (样本数, 特征数)")
print(f"   y_tensor 形状: {y_tensor.shape}  (样本数, 1)")

# ============================================================================
# 🏗️ 第四步：定义神经网络结构
# ============================================================================
# 🔑 这是整个代码最核心的部分！
#
# 🧠 生活比喻：搭积木
#    - nn.Module 是"积木底板"
#    - nn.Linear 是"连接积木"
#    - 激活函数是"转换器"
#
# 📊 网络结构图：
#
#    输入层(3)        隐藏层(8)         输出层(1)
#    ┌───┐           ┌───┐
#    │ x1├──────────►│ h1├──┐
#    └───┘     w     └───┘  │
#    ┌───┐    ╱╲     ┌───┐  │         ┌───┐
#    │ x2├───╱  ╲───►│ h2├──┼────────►│ y │  →  0~1 概率
#    └───┘  ╱    ╲   └───┘  │         └───┘
#    ┌───┐ ╱      ╲  ┌───┐  │
#    │ x3├╱        ╲►│...├──┘
#    └───┘          └───┘
#                    (8个)
#
#    x1=云量  x2=风速  x3=湿度          y=下雨概率
#
# ═══════════════════════════════════════════════════════════════════════════
# 【网络结构与数据定义的对应关系】
# ═══════════════════════════════════════════════════════════════════════════
#
# 一、输入层的 x1, x2, x3 与 X_tensor 的关系
#
#    回顾数据定义：
#        X_tensor.shape = (500, 3)
#
#    X_tensor 的每一行是一个样本，每一列是一个特征：
#
#        X_tensor = | x1⁽⁰⁾  x2⁽⁰⁾  x3⁽⁰⁾ |   ← 第 0 个样本
#                   | x1⁽¹⁾  x2⁽¹⁾  x3⁽¹⁾ |   ← 第 1 个样本
#                   | ...    ...    ...   |
#                   | x1⁽⁴⁹⁹⁾ x2⁽⁴⁹⁹⁾ x3⁽⁴⁹⁹⁾|   ← 第 499 个样本
#
#    对应关系：
#        x1 = X_tensor[:, 0] = 云量 (cloud)
#        x2 = X_tensor[:, 1] = 风速 (wind)
#        x3 = X_tensor[:, 2] = 湿度 (humidity)
#
#    当网络处理第 i 个样本时：
#        x1 = X_tensor[i, 0]  # 第 i 个样本的云量
#        x2 = X_tensor[i, 1]  # 第 i 个样本的风速
#        x3 = X_tensor[i, 2]  # 第 i 个样本的湿度
#
# 二、隐藏层的 h1, h2, ..., h8 是什么？
#
#    隐藏层的值是【计算出来的】，不是我们定义的数据。
#
#    计算公式（以单个样本为例）：
#
#        h = ReLU(x @ W₁ + b₁)
#
#    其中：
#        x = [x1, x2, x3]       # 输入向量，shape = (3,)
#        W₁ = fc1.weight        # 权重矩阵，shape = (8, 3)
#        b₁ = fc1.bias          # 偏置向量，shape = (8,)
#        h = [h1, h2, ..., h8]  # 隐藏层输出，shape = (8,)
#
#    展开来看每个隐藏神经元的计算：
#
#        h1 = ReLU(w₁₁×x1 + w₁₂×x2 + w₁₃×x3 + b₁)
#        h2 = ReLU(w₂₁×x1 + w₂₂×x2 + w₂₃×x3 + b₂)
#        ...
#        h8 = ReLU(w₈₁×x1 + w₈₂×x2 + w₈₃×x3 + b₈)
#
#    每个 hᵢ 是输入特征的加权求和，再经过 ReLU 激活。
#    权重 wᵢⱼ 是网络需要【学习】的参数。
#
# 三、输出层的 y 与 y_tensor 的关系
#
#    网络输出 ŷ（预测值）：
#
#        ŷ = Sigmoid(h @ W₂ + b₂)
#
#    其中：
#        h = [h1, ..., h8]      # 隐藏层输出，shape = (8,)
#        W₂ = fc2.weight        # 权重矩阵，shape = (1, 8)
#        b₂ = fc2.bias          # 偏置向量，shape = (1,)
#        ŷ ∈ [0, 1]             # 预测的下雨概率
#
#    y_tensor 是【真实标签】（ground truth）：
#        y_tensor.shape = (500, 1)
#        y_tensor[i] ∈ {0.0, 1.0}  # 第 i 个样本是否真的下雨
#
#    训练目标：让 ŷ 尽可能接近 y_tensor
#
# 四、完整的数据流（以第 i 个样本为例）
#
#    X_tensor[i] = [62.0, 39.0, 45.0]  (云量=62, 风速=39, 湿度=45)
#         │
#         │  x1=62.0, x2=39.0, x3=45.0
#         ▼
#    ┌─────────────────────────────────────────────────┐
#    │  fc1: h = ReLU([62, 39, 45] @ W₁ + b₁)          │
#    │       假设计算结果 h = [2.1, 0, 5.3, 0, ...]    │
#    │       (注意 ReLU 把负数变成了 0)                 │
#    └─────────────────────────────────────────────────┘
#         │
#         │  h1=2.1, h2=0, h3=5.3, ...
#         ▼
#    ┌─────────────────────────────────────────────────┐
#    │  fc2: ŷ = Sigmoid(h @ W₂ + b₂)                  │
#    │       假设计算结果 ŷ = 0.87                      │
#    └─────────────────────────────────────────────────┘
#         │
#         │  预测：87% 概率下雨
#         ▼
#    与 y_tensor[i] = 1.0 (真实标签：下雨) 比较
#         │
#         ▼
#    BCELoss 计算损失，反向传播更新 W₁, b₁, W₂, b₂
#
# 五、批量处理时的维度变化
#
#    实际训练时，我们一次处理所有 500 个样本（batch_size=500）：
#
#    输入：  X_tensor         (500, 3)   ← 500 个样本，每个 3 个特征
#              ↓
#    fc1:    X @ W₁.T + b₁    (500, 8)   ← 500 个样本，每个产生 8 个隐藏值
#              ↓
#    ReLU:   ReLU(...)        (500, 8)   ← 形状不变，只是把负数变 0
#              ↓
#    fc2:    H @ W₂.T + b₂    (500, 1)   ← 500 个样本，每个产生 1 个输出
#              ↓
#    Sigmoid: Sigmoid(...)    (500, 1)   ← 形状不变，压缩到 0~1
#              ↓
#    输出：  outputs          (500, 1)   ← 与 y_tensor (500, 1) 形状一致
#
#    这就是为什么 y_tensor 必须是 (500, 1) 而不是 (500,)！
#
# ═══════════════════════════════════════════════════════════════════════════

print("\n🏗️ [第2步] 搭建神经网络...")


class WeatherNet(nn.Module):
    """
    天气预测神经网络
    
    🔑 为什么要继承 nn.Module？
       - 自动追踪所有可学习参数
       - 提供 .parameters() 方法给优化器
       - 支持 .to(device) 切换 CPU/GPU
    """
    
    def __init__(self):
        """
        初始化网络结构 —— 就像画建筑图纸
        """
        super(WeatherNet, self).__init__()  # 调用父类初始化（必须的！）
        # ↑ 相当于告诉 PyTorch："我要开始定义网络了"
        
        # 🔑 nn.Linear(in_features, out_features) = 全连接层
        #    - 数学公式：y = x @ W + b
        #    - W 是权重矩阵，b 是偏置向量
        #    - "全连接"意思是：前一层每个神经元都连到后一层每个神经元
        
        self.fc1 = nn.Linear(3, 8)    # 第一层：3个输入 → 8个隐藏神经元
        #          ↑ fc = Fully Connected（全连接）
        #          ↑ 3 = 输入特征数（云量、风速、湿度）
        #          ↑ 8 = 隐藏层神经元数（可以调整，这是"超参数"）
        
        self.fc2 = nn.Linear(8, 1)    # 第二层：8个隐藏 → 1个输出
        #          ↑ 8 = 必须和 fc1 的输出匹配！
        #          ↑ 1 = 输出一个数（下雨概率）
        
        self.sigmoid = nn.Sigmoid()   # Sigmoid 激活函数
        # ↑ 把任意数值"压缩"到 0~1 之间，变成概率
        #
        # 📊 Sigmoid 函数图像：
        #         1 ──────────────╮
        #                       ╱
        #       0.5 ──────────╱──────
        #                   ╱
        #         0 ──────╯──────────
        #            -∞    0    +∞
        
        # 🆚 ReLU vs Sigmoid (它们的关系):
        #    虽然都是"激活函数"（负责引入非线性），但在现代神经网络中分工明确：
        #
        #    1. ReLU (Rectified Linear Unit): 
        #       - 位置：通常用于【隐藏层】。
        #       - 作用：负责"学习特征"。它保留正数信号，过滤负数噪声。
        #       - 优势：计算极快，且在深层网络中不会出现"梯度消失"（学不动）的问题。
        #
        #    2. Sigmoid: 
        #       - 位置：通常用于【输出层】(仅限二分类问题)。
        #       - 作用：负责"输出结果"。它把任意数值硬挤压到 0~1 之间，强制变成概率。
        #       - 劣势：如果在中间层大量使用，因为两端梯度几乎为0，会导致网络无法训练（梯度消失）。
        #
        #    🤝 总结：ReLU 是为了更好地"学"，Sigmoid 是为了规范地"出"。

        # 💡 为什么隐藏层用 8 个神经元？
        #    - 太少（如2个）：学习能力不足，欠拟合
        #    - 太多（如100个）：可能过拟合，也更慢
        #    - 8 是个平衡点，适合这个简单问题
    
    def forward(self, x):
        """
        前向传播 —— 数据从输入流向输出的过程
        
        🧠 生活比喻：工厂流水线
           原材料(x) → 第一道工序(fc1+relu) → 第二道工序(fc2+sigmoid) → 成品(概率)
        
        参数:
            x: 输入数据，shape=(batch_size, 3)
               batch_size = 一次处理多少样本
        
        返回:
            输出概率，shape=(batch_size, 1)
        """
        # 第一层：线性变换 + ReLU 激活
        x = self.fc1(x)           # 线性变换：x @ W1 + b1，shape: (N,3) → (N,8)
        x = torch.relu(x)         # ReLU 激活：负数变0，正数不变
        #
        # 🔑 为什么需要激活函数？
        #    没有激活函数，多层线性变换等于一层（线性的线性还是线性）
        #    有了激活函数，网络才能学习"非线性"关系
        #
        # 📊 ReLU 函数图像：
        #         y
        #         │    ╱
        #         │   ╱
        #         │  ╱
        #    ─────┴─╱────── x
        #         0
        #    ReLU(x) = max(0, x)
        #
        # ❓ 思考：为什么 ReLU 这么简单却很有效？
        #    1. 计算快（只是比较和取值）
        #    2. 缓解梯度消失问题
        #    3. 产生稀疏激活（很多0），类似生物神经元
        
        # 第二层：线性变换 + Sigmoid 激活
        x = self.fc2(x)           # 线性变换：x @ W2 + b2，shape: (N,8) → (N,1)
        x = self.sigmoid(x)       # Sigmoid：压缩到 0~1，变成概率
        
        return x  # 返回预测的下雨概率


# 🎯 创建网络实例
model = WeatherNet()

# 打印网络结构
print("\n   网络结构：")
print(f"   {model}")

# 查看参数数量
total_params = sum(p.numel() for p in model.parameters())
#
# ═══════════════════════════════════════════════════════════════════════════════
# 【模型参数统计的深度解析】
# ═══════════════════════════════════════════════════════════════════════════════
#
# 一、model.parameters() 返回什么？
#
#    这是一个 Python 生成器 (Generator)，惰性迭代所有可学习参数。
#
#    "可学习参数" = requires_grad=True 的 Tensor
#    包括：权重矩阵 (weight)、偏置向量 (bias)
#    不包括：输入数据、中间激活值、超参数
#
#    等价于遍历：
#        for name, param in model.named_parameters():
#            yield param  # 只返回参数，不返回名字
#
# 二、p.numel() 是什么？
#
#    numel = "number of elements"（元素个数）
#
#    对于 Tensor，numel() 返回所有维度的乘积：
#        tensor.shape = (3, 8)  →  numel() = 3 × 8 = 24
#        tensor.shape = (8,)    →  numel() = 8
#        tensor.shape = ()      →  numel() = 1（标量）
#
# 三、参数计算的数学推导
#
#    全连接层 nn.Linear(in_features, out_features) 的参数：
#
#    权重矩阵 W ∈ ℝ^{out × in}：
#        参数量 = out_features × in_features
#
#    偏置向量 b ∈ ℝ^{out}：
#        参数量 = out_features
#
#    总参数量 = out × in + out = out × (in + 1)
#
#    应用到本网络：
#
#    fc1: nn.Linear(3, 8)
#        W₁ ∈ ℝ^{8×3} → 8 × 3 = 24 个参数
#        b₁ ∈ ℝ^{8}   → 8 个参数
#        小计：24 + 8 = 32 个参数
#
#    fc2: nn.Linear(8, 1)
#        W₂ ∈ ℝ^{1×8} → 1 × 8 = 8 个参数
#        b₂ ∈ ℝ^{1}   → 1 个参数
#        小计：8 + 1 = 9 个参数
#
#    总计：32 + 9 = 41 个参数
#
# 四、参数量与模型容量的关系
#
#    参数量 ↔ 模型容量 ↔ 过拟合风险
#
#    经验法则（来自统计学习理论）：
#        训练样本数 ≥ 5~10 × 参数量
#
#    本例：
#        样本数 = 500
#        参数量 = 41
#        比值 = 500 / 41 ≈ 12.2 ✓（足够）
#
#    如果参数量过多（如 10000），500 个样本将严重不足，导致过拟合。
#
# 五、现代网络的参数量级
#
#    本网络：        41 参数              （教学示例）
#    LeNet-5：       ~60K 参数            （1998，手写数字）
#    AlexNet：       ~60M 参数            （2012，ImageNet 冠军）
#    VGG-16：        ~138M 参数           （2014，深层网络）
#    ResNet-152：    ~60M 参数            （2015，残差连接）
#    GPT-3：         175B 参数            （2020，大语言模型）
#    GPT-4：         ~1.8T 参数（估计）   （2023，多模态）
#
# ═══════════════════════════════════════════════════════════════════════════════

print(f"\n   总参数量: {total_params}")
print("   参数分布:")
for name, param in model.named_parameters():
    print(f"      {name}: {param.shape} = {param.numel()} 个参数")
#
# model.named_parameters() 返回 (name, param) 元组的生成器
#
# 输出示例：
#    fc1.weight: torch.Size([8, 3]) = 24 个参数
#    fc1.bias: torch.Size([8]) = 8 个参数
#    fc2.weight: torch.Size([1, 8]) = 8 个参数
#    fc2.bias: torch.Size([1]) = 1 个参数
#
# 注意：Sigmoid 层没有可学习参数（纯函数变换）

# ============================================================================
# ⚙️ 第五步：定义损失函数和优化器
# ============================================================================
# 🧠 生活比喻：
#    - 损失函数 = 老师的评分标准（告诉你错得多离谱）
#    - 优化器 = 学习方法（怎么根据错误来改进）

print("\n⚙️ [第3步] 配置训练工具...")

# ----------------------------------------------------------------------------
# 损失函数：Binary Cross Entropy（二元交叉熵）
# ----------------------------------------------------------------------------
#
# ═══════════════════════════════════════════════════════════════════════════
# 【二元交叉熵损失函数的深度解析】
# ═══════════════════════════════════════════════════════════════════════════
#
# 一、数学定义
#
#    对于单个样本，BCE 定义为：
#
#        L = -[y × log(ŷ) + (1-y) × log(1-ŷ)]
#
#    其中：
#    - y ∈ {0, 1} 是真实标签
#    - ŷ ∈ (0, 1) 是预测概率
#
#    对于 n 个样本，总损失是各样本损失的平均：
#
#        L_total = (1/n) × Σᵢ L_i
#
# 二、为什么叫"交叉熵"？
#
#    这源于信息论。熵是衡量信息不确定性的指标。
#
#    - 真实分布 P：y=1 的概率为 p，y=0 的概率为 1-p
#    - 预测分布 Q：y=1 的概率为 q，y=0 的概率为 1-q
#
#    交叉熵 H(P, Q) = -Σ P(x) × log(Q(x))
#                   = -[p × log(q) + (1-p) × log(1-q)]
#
#    在二分类中，真实分布是退化的（y=1 或 y=0），代入得到 BCE 公式。
#
# 三、BCE vs MSE 的对比
#
#    假设真实标签 y = 1：
#
#    预测 ŷ    |  MSE = (1-ŷ)²   |  BCE = -log(ŷ)
#    ─────────────────────────────────────────────
#    0.9       |  0.01           |  0.105
#    0.7       |  0.09           |  0.357
#    0.5       |  0.25           |  0.693
#    0.3       |  0.49           |  1.204
#    0.1       |  0.81           |  2.303
#    0.01      |  0.98           |  4.605
#
#    关键观察：
#    - MSE 在预测接近 0 时增长缓慢（0.81 vs 0.98）
#    - BCE 在预测接近 0 时急剧增长（2.303 → 4.605）
#
#    这意味着 BCE 对"自信的错误"惩罚更严厉。
#    当模型预测 ŷ=0.01（非常自信地说"不会下雨"）但实际下雨时，
#    BCE 给出的损失是 MSE 的 4.7 倍。
#
# 四、BCE 与 Sigmoid 的配合
#
#    BCE + Sigmoid 的组合有美妙的数学性质。
#
#    令 z = W @ h + b（Sigmoid 之前的值），则：
#
#        ∂L/∂z = ŷ - y
#
#    这是一个极其简洁的梯度！计算高效且数值稳定。
#
#    PyTorch 提供了 nn.BCEWithLogitsLoss，将 Sigmoid 和 BCE 合并，
#    利用了 log-sum-exp 技巧避免数值溢出，推荐在生产环境使用。
#
# ═══════════════════════════════════════════════════════════════════════════

criterion = nn.BCELoss()  # Binary Cross Entropy Loss
print(f"   损失函数: BCELoss (二元交叉熵)")

# ----------------------------------------------------------------------------
# 优化器：Adam（自适应矩估计）
# ----------------------------------------------------------------------------
# 🔑 优化器负责"更新权重"
#
# 基本公式：新权重 = 旧权重 - 学习率 × 梯度
#
# ❌ 普通 SGD 的问题：
#    - 学习率固定，有时太大有时太小
#    - 所有参数用同样的学习率
#
# ✅ Adam 的优点：
#    - 自适应调整每个参数的学习率
#    - 结合了 Momentum（惯性）和 RMSprop（自适应）
#    - 大多数情况下"开箱即用"
#
# 📊 不同优化器的形象比喻：
#    SGD：骑自行车下山（可能绕远路）
#    Momentum：滑雪下山（有惯性，更顺滑）
#    Adam：自动驾驶下山（智能调整方向和速度）

optimizer = optim.Adam(model.parameters(), lr=0.01)
#
# ═══════════════════════════════════════════════════════════════════════════════
# 【Adam 优化器的完整数学推导】
# ═══════════════════════════════════════════════════════════════════════════════
#
# Adam = Adaptive Moment Estimation（自适应矩估计）
# 论文：Kingma & Ba, 2014, "Adam: A Method for Stochastic Optimization"
# 引用数：200,000+（深度学习最高引用论文之一）
#
# 一、算法伪代码
#
#    输入：学习率 α (默认 0.001)、衰减率 β₁=0.9, β₂=0.999、ε=1e-8
#    初始化：m₀=0, v₀=0, t=0
#
#    while 未收敛 do:
#        t ← t + 1
#        g_t ← ∇_θ L(θ_{t-1})           # 计算梯度
#        m_t ← β₁·m_{t-1} + (1-β₁)·g_t   # 更新一阶矩（均值）
#        v_t ← β₂·v_{t-1} + (1-β₂)·g_t²  # 更新二阶矩（方差）
#        m̂_t ← m_t / (1-β₁^t)            # 偏差校正
#        v̂_t ← v_t / (1-β₂^t)            # 偏差校正
#        θ_t ← θ_{t-1} - α·m̂_t / (√v̂_t + ε)  # 更新参数
#
# 二、各项的物理意义
#
#    m_t（一阶矩）：梯度的指数移动平均
#        - 类似「动量」，平滑梯度的历史
#        - 减少随机梯度的方差，避免震荡
#        - β₁=0.9 表示：当前梯度占 10%，历史占 90%
#
#    v_t（二阶矩）：梯度平方的指数移动平均
#        - 估计梯度的「方差」
#        - 梯度大的参数，有效学习率变小（自适应）
#        - β₂=0.999 表示：变化非常缓慢，稳定估计
#
#    偏差校正：
#        - 初始时 m₀=0, v₀=0，会导致估计偏向 0
#        - 除以 (1-β^t) 可修正这种偏差
#        - 当 t 很大时，校正项趋近于 1
#
# 三、为什么 Adam 效果好？
#
#    1. 自适应学习率
#       - 梯度大的参数：v_t 大 → √v̂_t 大 → 有效学习率小
#       - 梯度小的参数：v_t 小 → √v̂_t 小 → 有效学习率大
#       - 自动平衡各参数的更新幅度
#
#    2. 动量效应
#       - m_t 积累了历史梯度信息
#       - 在正确方向上加速，在错误方向上减速
#       - 帮助逃离「鞍点」（Saddle Point）
#
#    3. 对超参数不敏感
#       - 默认参数 (α=0.001, β₁=0.9, β₂=0.999) 几乎适用于所有任务
#       - 是「开箱即用」的首选优化器
#
# 四、学习率 α 的选择
#
#    常见取值范围：1e-4 ~ 1e-2
#
#    α = 0.01（本例）：
#        - 相对较大，适合小网络快速收敛
#        - 风险：可能错过最优解
#
#    α = 0.001（默认）：
#        - 更保守，适合大多数任务
#        - 配合学习率衰减使用效果更好
#
#    α = 1e-5 ~ 1e-4：
#        - 微调预训练模型（Fine-tuning）时使用
#        - 避免破坏已学到的特征
#
# 五、参数传递解析
#
#    optim.Adam(model.parameters(), lr=0.01)
#               │                    │
#               │                    └─► lr: 初始学习率
#               │
#               └─► params: 要优化的参数迭代器
#                   model.parameters() 返回所有 requires_grad=True 的参数
#
#    其他可选参数：
#        betas=(0.9, 0.999)   # 一阶和二阶矩的衰减率
#        eps=1e-8             # 数值稳定性常数（防止除零）
#        weight_decay=0       # L2 正则化系数（AdamW 建议用）
#        amsgrad=False        # 是否使用 AMSGrad 变体
#
# ═══════════════════════════════════════════════════════════════════════════════

print(f"   优化器: Adam (学习率=0.01)")

# ============================================================================
# 🔄 第六步：训练循环 —— 神经网络"学习"的过程
# ============================================================================
# 🔑 这是理解深度学习的关键！
#
# 📊 训练流程图：
#
#    ┌─────────────────────────────────────────────────────────────┐
#    │                      一个 Epoch 的流程                       │
#    │                                                             │
#    │  ┌──────────┐    ┌──────────┐    ┌──────────┐             │
#    │  │ 1.前向传播 │───►│ 2.计算损失│───►│ 3.反向传播│             │
#    │  │ (预测)   │    │ (打分)   │    │ (找原因) │             │
#    │  └──────────┘    └──────────┘    └──────────┘             │
#    │       │                               │                    │
#    │       │         ┌──────────┐          │                    │
#    │       └─────────│ 4.更新权重│◄─────────┘                    │
#    │                 │ (改进)   │                               │
#    │                 └──────────┘                               │
#    │                      │                                     │
#    │                      ▼                                     │
#    │               重复 N 个 epoch                               │
#    └─────────────────────────────────────────────────────────────┘
#
# 🧠 生活比喻：学生考试
#    1. 前向传播 = 学生答题（模型给出预测）
#    2. 计算损失 = 老师打分（对比预测和真实答案，算出差距）
#    3. 反向传播 = 分析错误原因（PyTorch 自动计算每个参数对错误的贡献/梯度）
#    4. 更新权重 = 改正错误认知（根据梯度调整参数，争取下次少犯错）

print("\n🔄 [第4步] 开始训练...\n")

epochs = 50  # 训练轮数
# 💡 epoch = 把所有数据过一遍
# ❓ 为什么要多个 epoch？
#    一遍记不住！就像复习，多看几遍才能记牢

# 记录训练历史（用于分析）
loss_history = []

for epoch in range(epochs):
    # ========== 步骤 1: 前向传播 ==========
    # 把数据"喂"给网络，得到预测结果
    outputs = model(X_tensor)  # shape: (500, 1)
    #
    # ═══════════════════════════════════════════════════════════════════════
    # 【前向传播的逐步计算过程】
    # ═══════════════════════════════════════════════════════════════════════
    #
    # 一、model(X_tensor) 等价于 model.forward(X_tensor)
    #
    #    PyTorch 的 nn.Module 重载了 __call__ 方法：
    #    - 调用 model(x) 时，实际执行的是 model.__call__(x)
    #    - __call__ 内部会调用 self.forward(x)
    #    - 还会处理 hooks（钩子函数）等额外逻辑
    #
    # 二、前向传播的数据流（以第 0 个样本为例）
    #
    #    假设 X_tensor[0] = [51.0, 92.0, 71.0]（云量=51, 风速=92, 湿度=71）
    #
    #    步骤 1：进入 fc1 层
    #    ─────────────────
    #        输入：x = [51.0, 92.0, 71.0]     shape: (3,)
    #        权重：W₁ = fc1.weight             shape: (8, 3)
    #        偏置：b₁ = fc1.bias               shape: (8,)
    #
    #        计算：z₁ = x @ W₁.T + b₁
    #
    #        展开：z₁[0] = 51.0×W₁[0,0] + 92.0×W₁[0,1] + 71.0×W₁[0,2] + b₁[0]
    #              z₁[1] = 51.0×W₁[1,0] + 92.0×W₁[1,1] + 71.0×W₁[1,2] + b₁[1]
    #              ...
    #              z₁[7] = 51.0×W₁[7,0] + 92.0×W₁[7,1] + 71.0×W₁[7,2] + b₁[7]
    #
    #        假设结果：z₁ = [-2.3, 1.5, 0.8, -0.1, 3.2, -1.7, 2.1, 0.3]
    #
    #    步骤 2：经过 ReLU 激活
    #    ─────────────────────
    #        ReLU(x) = max(0, x)
    #
    #        z₁   = [-2.3, 1.5, 0.8, -0.1, 3.2, -1.7, 2.1, 0.3]
    #                  ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓
    #        h    = [ 0.0, 1.5, 0.8,  0.0, 3.2,  0.0, 2.1, 0.3]
    #
    #        注意：负数全部变成 0，正数保持不变
    #
    #    步骤 3：进入 fc2 层
    #    ─────────────────
    #        输入：h = [0.0, 1.5, 0.8, 0.0, 3.2, 0.0, 2.1, 0.3]   shape: (8,)
    #        权重：W₂ = fc2.weight                                 shape: (1, 8)
    #        偏置：b₂ = fc2.bias                                   shape: (1,)
    #
    #        计算：z₂ = h @ W₂.T + b₂
    #             z₂ = 0.0×W₂[0] + 1.5×W₂[1] + ... + 0.3×W₂[7] + b₂
    #
    #        假设结果：z₂ = 1.87
    #
    #    步骤 4：经过 Sigmoid 激活
    #    ───────────────────────
    #        σ(z) = 1 / (1 + e^(-z))
    #        σ(1.87) = 1 / (1 + e^(-1.87))
    #                = 1 / (1 + 0.154)
    #                = 1 / 1.154
    #                = 0.866
    #
    #        输出：ŷ = 0.866（预测 86.6% 概率下雨）
    #
    # 三、批量处理时的矩阵运算
    #
    #    实际上 500 个样本是【同时】计算的：
    #
    #        X_tensor: (500, 3)   ← 500 个样本，每个 3 个特征
    #            │
    #            ▼  fc1: X @ W₁.T + b₁
    #        (500, 8)             ← 500 个样本，每个 8 个隐藏值
    #            │
    #            ▼  ReLU
    #        (500, 8)             ← 形状不变
    #            │
    #            ▼  fc2: H @ W₂.T + b₂
    #        (500, 1)             ← 500 个样本，每个 1 个输出
    #            │
    #            ▼  Sigmoid
    #        (500, 1)             ← 500 个概率值
    #
    #    这就是 outputs 的最终形状：(500, 1)
    #
    # 四、outputs 的具体内容
    #
    #    outputs = | 0.866 |   ← 样本 0 的预测概率
    #              | 0.123 |   ← 样本 1 的预测概率
    #              | 0.945 |   ← 样本 2 的预测概率
    #              |  ...  |
    #              | 0.567 |   ← 样本 499 的预测概率
    #
    #    每个值都在 [0, 1] 范围内（Sigmoid 的输出范围）
    #
    # ═══════════════════════════════════════════════════════════════════════
    
    # ========== 步骤 2: 计算损失 ==========
    # 比较预测值和真实值，计算"错了多少"
    loss = criterion(outputs, y_tensor)
    #
    # ═══════════════════════════════════════════════════════════════════════
    # 【损失计算的逐步过程】
    # ═══════════════════════════════════════════════════════════════════════
    #
    # 一、输入数据
    #
    #    outputs (预测值):        y_tensor (真实标签):
    #    ┌───────┐                ┌───────┐
    #    │ 0.866 │                │ 1.0   │   ← 样本 0：预测 0.866，实际下雨
    #    │ 0.123 │                │ 0.0   │   ← 样本 1：预测 0.123，实际不下雨
    #    │ 0.945 │                │ 1.0   │   ← 样本 2：预测 0.945，实际下雨
    #    │  ...  │                │  ...  │
    #    │ 0.567 │                │ 1.0   │   ← 样本 499
    #    └───────┘                └───────┘
    #    shape: (500, 1)          shape: (500, 1)
    #
    # 二、BCE 损失公式
    #
    #    对于单个样本 i：
    #        L_i = -[y_i × log(ŷ_i) + (1-y_i) × log(1-ŷ_i)]
    #
    #    根据 y_i 的值，公式简化为：
    #        - 若 y_i = 1（下雨）：  L_i = -log(ŷ_i)
    #        - 若 y_i = 0（不下雨）：L_i = -log(1 - ŷ_i)
    #
    # 三、逐样本计算示例
    #
    #    样本 0：y=1.0, ŷ=0.866
    #        L₀ = -log(0.866) = 0.144
    #        解读：预测接近 1，损失小 ✓
    #
    #    样本 1：y=0.0, ŷ=0.123
    #        L₁ = -log(1 - 0.123) = -log(0.877) = 0.131
    #        解读：预测接近 0，损失小 ✓
    #
    #    假设样本 k：y=1.0, ŷ=0.1（严重预测错误）
    #        L_k = -log(0.1) = 2.303
    #        解读：实际下雨却预测概率很低，损失很大 ✗
    #
    # 四、总损失的计算
    #
    #    BCELoss 默认使用 'mean' 规约：
    #
    #        loss = (1/n) × Σᵢ L_i
    #             = (L₀ + L₁ + L₂ + ... + L₄₉₉) / 500
    #
    #    假设计算结果：loss = 0.4523
    #
    # 五、loss 的数据类型
    #
    #    loss 是一个 0 维 Tensor（标量）：
    #        loss.shape = torch.Size([])   # 空的 shape 表示标量
    #        loss.item() = 0.4523          # 转成 Python float
    #        loss.requires_grad = True     # 需要计算梯度
    #
    # 六、loss 的数值范围
    #
    #    理论上：
    #        - 最小值：0（完美预测，所有 ŷ_i 都等于 y_i）
    #        - 最大值：∞（当 ŷ→0 而 y=1，或 ŷ→1 而 y=0）
    #
    #    实际训练中：
    #        - 初始：通常在 0.6~0.7（随机猜测）
    #        - 收敛后：通常在 0.1~0.3（学会了规律）
    #
    # ═══════════════════════════════════════════════════════════════════════
    
    # ========== 步骤 3: 梯度清零 ==========
    # ⚠️ 这步很容易忘！PyTorch 默认会累积梯度
    optimizer.zero_grad()
    #
    # ═══════════════════════════════════════════════════════════════════════════
    # 【梯度清零的深入理解】
    # ═══════════════════════════════════════════════════════════════════════════
    #
    # 一、为什么需要清零？
    #
    #    PyTorch 的设计哲学：梯度默认【累加】而非覆盖。
    #
    #    这个设计看似奇怪，实则有深刻的工程考量：
    #    - 某些优化算法（如梯度累积）需要多个 mini-batch 的梯度求和
    #    - 当显存不足以一次处理大 batch 时，可以分多次累加梯度
    #    - RNN 等序列模型可能需要跨时间步累加梯度
    #
    #    但对于标准训练，我们每次迭代只需要当前 batch 的梯度。
    #    若不清零，梯度会不断累积，导致：
    #    - 梯度值异常增大
    #    - 权重更新幅度失控
    #    - 网络发散（loss 变成 NaN）
    #
    # 二、清零的时机
    #
    #    本代码的顺序：forward → loss → zero_grad → backward → step
    #
    #    更常见的顺序：zero_grad → forward → loss → backward → step
    #
    #    两种顺序都是正确的！关键是确保：
    #    zero_grad() 在 backward() 之前被调用
    #
    #    因为 backward() 会【累加】梯度到 .grad 属性，
    #    所以必须在累加前把旧梯度清空。
    #
    # 三、技术细节
    #
    #    zero_grad() 做了什么？
    #    - 遍历 model.parameters() 中的每个参数
    #    - 将每个参数的 .grad 属性设为 None 或全零张量
    #
    #    PyTorch 1.7+ 引入了 set_to_none=True 选项：
    #    optimizer.zero_grad(set_to_none=True)
    #    - 直接将 .grad 设为 None 而非全零张量
    #    - 略微节省内存，但某些情况下可能引起问题
    #
    # ═══════════════════════════════════════════════════════════════════════════
    
    # ========== 步骤 4: 反向传播 ==========
    # 计算每个参数的梯度（偏导数）
    loss.backward()
    #
    # ═══════════════════════════════════════════════════════════════════════════
    # 【反向传播的数学原理】
    # ═══════════════════════════════════════════════════════════════════════════
    #
    # 一、问题的本质
    #
    #    我们需要计算损失函数 L 对每个参数的偏导数：
    #
    #        ∂L/∂W₁, ∂L/∂b₁, ∂L/∂W₂, ∂L/∂b₂
    #
    #    这些梯度告诉我们："如果稍微改变某个参数，损失会如何变化？"
    #
    # 二、链式法则的应用
    #
    #    本网络的计算图：
    #
    #        X → [fc1] → h₁ → [ReLU] → h₂ → [fc2] → z → [Sigmoid] → ŷ → [BCE] → L
    #
    #    以 ∂L/∂W₁ 为例，需要沿路径逆向应用链式法则：
    #
    #        ∂L/∂W₁ = ∂L/∂ŷ × ∂ŷ/∂z × ∂z/∂h₂ × ∂h₂/∂h₁ × ∂h₁/∂W₁
    #
    #    其中：
    #    - ∂L/∂ŷ  = BCE 损失的导数 = (ŷ - y) / (ŷ(1-ŷ))
    #    - ∂ŷ/∂z  = Sigmoid 的导数 = ŷ(1-ŷ)
    #    - ∂z/∂h₂ = fc2 的导数 = W₂
    #    - ∂h₂/∂h₁ = ReLU 的导数 = 1 (若 h₁>0) 或 0 (若 h₁≤0)
    #    - ∂h₁/∂W₁ = fc1 的导数 = X
    #
    # 三、PyTorch 的自动微分
    #
    #    PyTorch 在前向传播时构建了【计算图】。
    #    每个 Tensor 都记录了它是如何被计算出来的。
    #
    #    调用 loss.backward() 时：
    #    1. 从 loss 节点出发
    #    2. 沿计算图逆向遍历
    #    3. 对每条边应用链式法则
    #    4. 将计算出的梯度存入各参数的 .grad 属性
    #
    #    执行完后：
    #    - model.fc1.weight.grad 存储了 ∂L/∂W₁
    #    - model.fc1.bias.grad 存储了 ∂L/∂b₁
    #    - model.fc2.weight.grad 存储了 ∂L/∂W₂
    #    - model.fc2.bias.grad 存储了 ∂L/∂b₂
    #
    # 四、为什么这很重要？
    #
    #    手动推导这些梯度是可行的（对于简单网络），但：
    #    - 容易出错
    #    - 修改网络结构后需要重新推导
    #    - 复杂网络（如 ResNet-152）几乎不可能手算
    #
    #    自动微分让我们可以专注于设计网络结构，而非纠结于数学推导。
    #
    # ═══════════════════════════════════════════════════════════════════════════
    
    # ========== 步骤 5: 更新权重 ==========
    # 根据梯度调整参数
    optimizer.step()
    #
    # ═══════════════════════════════════════════════════════════════════════════
    # 【权重更新的数学过程】
    # ═══════════════════════════════════════════════════════════════════════════
    #
    # 一、基础梯度下降
    #
    #    最简单的更新公式（SGD）：
    #
    #        W_new = W_old - η × ∂L/∂W
    #
    #    其中 η 是学习率。
    #
    #    直觉：沿着损失下降最快的方向走一小步。
    #
    # 二、Adam 优化器的改进
    #
    #    Adam (Adaptive Moment Estimation) 维护两个状态：
    #
    #    1. 一阶矩 m（梯度的指数移动平均）：
    #       m_t = β₁ × m_{t-1} + (1-β₁) × g_t
    #
    #    2. 二阶矩 v（梯度平方的指数移动平均）：
    #       v_t = β₂ × v_{t-1} + (1-β₂) × g_t²
    #
    #    其中 g_t = ∂L/∂W 是当前梯度。
    #
    #    更新公式（简化版）：
    #
    #        W_new = W_old - η × m_t / (√v_t + ε)
    #
    #    默认超参数：β₁=0.9, β₂=0.999, ε=1e-8
    #
    # 三、Adam 的优势
    #
    #    1. 自适应学习率：
    #       - 对于梯度较大的参数，实际学习率会变小
    #       - 对于梯度较小的参数，实际学习率会变大
    #       - 这使得训练更加稳定
    #
    #    2. 动量效应：
    #       - m_t 平滑了梯度的历史
    #       - 避免因单次异常梯度导致的震荡
    #
    #    3. 对超参数不敏感：
    #       - 默认参数在大多数任务上表现良好
    #       - 是深度学习中最常用的优化器之一
    #
    # ═══════════════════════════════════════════════════════════════════════════
    
    # 记录损失
    loss_history.append(loss.item())
    #
    # ═══════════════════════════════════════════════════════════════════════
    # 【loss.item() 和 loss_history 详解】
    # ═══════════════════════════════════════════════════════════════════════
    #
    # 一、为什么需要 .item()？
    #
    #    loss 是一个 PyTorch Tensor，带有计算图信息：
    #        type(loss) = <class 'torch.Tensor'>
    #        loss.requires_grad = True
    #        loss.grad_fn = <BinaryCrossEntropyBackward>
    #
    #    如果直接把 loss 存入列表：
    #        loss_history.append(loss)  # ← 错误做法！
    #
    #    问题：
    #        - 每个 loss 都持有整个计算图的引用
    #        - 50 个 epoch 会保留 50 个完整的计算图
    #        - 内存会不断增长，可能导致 OOM（内存溢出）
    #
    #    正确做法：
    #        loss_history.append(loss.item())  # ← 只保存数值
    #
    # 二、.item() 做了什么？
    #
    #    将 0 维 Tensor 转换为 Python 标量：
    #
    #        loss                    → tensor(0.4523, grad_fn=...)
    #        loss.item()             → 0.4523 (Python float)
    #
    #    转换后：
    #        - 不再与计算图关联
    #        - 可以安全地长期存储
    #        - 可以用于绘图、打印等
    #
    # 三、loss_history 的用途
    #
    #    训练结束后，loss_history 包含每个 epoch 的损失值：
    #
    #        loss_history = [0.693,   # epoch 0：初始，接近随机猜测
    #                        0.612,   # epoch 1：开始学习
    #                        0.534,   # epoch 2：继续下降
    #                        ...
    #                        0.142]   # epoch 49：收敛
    #
    #    可以用它来：
    #        1. 绘制损失曲线：plt.plot(loss_history)
    #        2. 判断是否收敛：loss_history[-1] < threshold
    #        3. 检测过拟合：观察是否先降后升
    #
    # 四、为什么 0.693 是初始损失？
    #
    #    未训练的网络近似于随机猜测（输出约 0.5）：
    #        BCE(0.5, 0) = -log(0.5) = 0.693
    #        BCE(0.5, 1) = -log(0.5) = 0.693
    #
    #    所以初始损失约等于 ln(2) ≈ 0.693
    #
    # ═══════════════════════════════════════════════════════════════════════
    
    # 每 5 轮打印一次进度
    if (epoch + 1) % 5 == 0:
        # 计算训练准确率
        with torch.no_grad():  # 临时关闭梯度计算（省内存、省时间）
            #
            # ═══════════════════════════════════════════════════════════════
            # 【准确率计算的逐步拆解】
            # ═══════════════════════════════════════════════════════════════
            #
            # 一、with torch.no_grad() 的作用
            #
            #    在这个代码块内：
            #    - 所有 Tensor 操作都不会构建计算图
            #    - 不会记录 grad_fn
            #    - 节省内存（不用存储中间变量）
            #    - 计算更快（跳过梯度追踪逻辑）
            #
            #    为什么这里可以关闭？
            #    - 我们只是在【评估】模型，不需要更新权重
            #    - 反向传播已经完成，梯度已经计算并应用
            #
            # 二、阈值判断：outputs >= 0.5
            #
            #    outputs（概率）：     outputs >= 0.5（布尔）：
            #    ┌───────┐             ┌───────┐
            #    │ 0.866 │             │ True  │   ← 0.866 >= 0.5
            #    │ 0.123 │      →      │ False │   ← 0.123 < 0.5
            #    │ 0.945 │             │ True  │   ← 0.945 >= 0.5
            #    │ 0.312 │             │ False │   ← 0.312 < 0.5
            #    └───────┘             └───────┘
            #
            #    阈值 0.5 的含义：
            #    - >= 0.5：模型认为"会下雨"
            #    - < 0.5：模型认为"不会下雨"
            #
            # 三、类型转换：.float()
            #
            #    布尔 Tensor：           Float Tensor：
            #    ┌───────┐               ┌───────┐
            #    │ True  │               │ 1.0   │
            #    │ False │       →       │ 0.0   │
            #    │ True  │               │ 1.0   │
            #    │ False │               │ 0.0   │
            #    └───────┘               └───────┘
            #
            #    为什么要转换？
            #    - 布尔值不能直接与 y_tensor 比较（类型不匹配）
            #    - 后续 mean() 需要数值类型
            #
            predictions = (outputs >= 0.5).float()
            #
            # 四、正确性判断：predictions == y_tensor
            #
            #    predictions：    y_tensor：      相等？
            #    ┌───────┐       ┌───────┐       ┌───────┐
            #    │ 1.0   │       │ 1.0   │       │ True  │  ← 预测正确
            #    │ 0.0   │   ==  │ 0.0   │   =   │ True  │  ← 预测正确
            #    │ 1.0   │       │ 1.0   │       │ True  │  ← 预测正确
            #    │ 0.0   │       │ 1.0   │       │ False │  ← 预测错误！
            #    └───────┘       └───────┘       └───────┘
            #
            # 五、计算准确率：.float().mean()
            #
            #    布尔 Tensor：           Float Tensor：
            #    ┌───────┐               ┌───────┐
            #    │ True  │               │ 1.0   │
            #    │ True  │       →       │ 1.0   │
            #    │ True  │               │ 1.0   │
            #    │ False │               │ 0.0   │
            #    └───────┘               └───────┘
            #
            #    mean() = (1.0 + 1.0 + 1.0 + 0.0) / 4 = 0.75
            #
            #    含义：4 个样本中有 3 个预测正确，准确率 75%
            #
            # 六、完整计算示例（假设 500 个样本）
            #
            #    假设：
            #    - 正确预测：423 个（True → 1.0）
            #    - 错误预测：77 个（False → 0.0）
            #
            #    accuracy = (423 × 1.0 + 77 × 0.0) / 500
            #             = 423 / 500
            #             = 0.846
            #
            #    准确率 84.6%
            #
            # ═══════════════════════════════════════════════════════════════
            #
            accuracy = (predictions == y_tensor).float().mean()
        
        print(f"   Epoch [{epoch+1:2d}/{epochs}]  "
              f"Loss: {loss.item():.4f}  "
              f"Accuracy: {accuracy.item()*100:.1f}%")

# 打印训练完成信息
print(f"\n   ✅ 训练完成！最终损失: {loss_history[-1]:.4f}")

# ============================================================================
# 🔬 额外演示：查看 Tensor 的核心属性
# ============================================================================
# 这部分演示注释中提到的 Tensor 五大属性：.data, .grad, .grad_fn, .device, .dtype

print("\n" + "=" * 60)
print("🔬 [演示] Tensor 的核心属性")
print("=" * 60)

# 1. 查看模型权重的属性
print("\n📊 模型第一层权重 (model.fc1.weight) 的属性：")
w = model.fc1.weight
print(f"   .data    (底层数据)     : shape={w.data.shape}, 前3个值={w.data[0, :3].tolist()}")
print(f"   .grad    (梯度)         : shape={w.grad.shape if w.grad is not None else 'None'}")
print(f"   .grad_fn (计算图父节点) : {w.grad_fn}")  # 参数没有 grad_fn，只有中间结果有
print(f"   .device  (存储位置)     : {w.device}")
print(f"   .dtype   (数据类型)     : {w.dtype}")
print(f"   .requires_grad          : {w.requires_grad}")

# 2. 查看中间计算结果的属性（有 grad_fn）
print("\n📊 前向传播中间结果的属性：")
# 做一次前向传播，查看中间 Tensor
sample_input = X_tensor[:1]  # 取第一个样本
intermediate = model.fc1(sample_input)  # fc1 的输出（ReLU 之前）
print(f"   中间结果 intermediate = fc1(x)")
print(f"   .data    (底层数据)     : {intermediate.data}")
print(f"   .grad_fn (计算图父节点) : {intermediate.grad_fn}")  # ← 这里有值！
print(f"   .device  (存储位置)     : {intermediate.device}")
print(f"   .dtype   (数据类型)     : {intermediate.dtype}")

# 3. 查看损失的属性
print("\n📊 损失值 (loss) 的属性：")
# 重新计算一次 loss 来演示
with torch.enable_grad():
    demo_output = model(X_tensor[:5])
    demo_loss = criterion(demo_output, y_tensor[:5])
print(f"   loss 的值              : {demo_loss.item():.4f}")
print(f"   .grad_fn (计算图父节点) : {demo_loss.grad_fn}")
print(f"   .requires_grad          : {demo_loss.requires_grad}")

# 4. 对比：普通 Tensor vs 需要梯度的 Tensor
print("\n📊 对比：普通 Tensor vs 需要梯度的 Tensor")
normal_tensor = torch.tensor([1.0, 2.0, 3.0])
grad_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
print(f"   普通 Tensor:    requires_grad={normal_tensor.requires_grad}, grad_fn={normal_tensor.grad_fn}")
print(f"   需要梯度 Tensor: requires_grad={grad_tensor.requires_grad}, grad_fn={grad_tensor.grad_fn}")

# 计算后产生 grad_fn
result = grad_tensor * 2 + 1
print(f"   计算后的结果:   requires_grad={result.requires_grad}, grad_fn={result.grad_fn}")

print("\n" + "=" * 60)

# ============================================================================
# 🧪 第七步：测试模型 —— 看看学到了什么
# ============================================================================
# 🔑 训练时用的数据，测试时要用"新"数据
#    （这里简化了，实际应该分训练集/测试集）
#
# 💡 验证时刻：
#    我们用之前设定的"上帝规则"（湿度+云量 > 风速*1.2）来验证模型是否真的学会了。
#    虽然模型从未见过这个公式，但它通过大量数据训练，应该能拟合出类似的决策边界。

print("\n🧪 [第5步] 测试模型...\n")

# with torch.no_grad() 的作用：
# 1. 告诉 PyTorch "我只是在预测，不需要计算梯度"（现在是考试时间，不是学习时间）
# 2. 省内存（不用存储中间值）
# 3. 计算更快
with torch.no_grad():
    # 创建一个测试样本
    test_sample = torch.tensor([[80, 10, 70]], dtype=torch.float32)
    #
    # ═══════════════════════════════════════════════════════════════════════
    # 【测试样本的创建详解】
    # ═══════════════════════════════════════════════════════════════════════
    #
    # 一、torch.tensor() 的输入格式
    #
    #    [[80, 10, 70]] 是一个嵌套列表：
    #    - 外层列表：[ ... ]      → 表示 batch 维度
    #    - 内层列表：[80, 10, 70] → 表示特征维度
    #
    #    结果形状：(1, 3) → 1 个样本，3 个特征
    #
    # 二、为什么需要二维形状？
    #
    #    神经网络期望的输入形状是 (batch_size, features)：
    #
    #    ✗ 错误：[80, 10, 70]        shape: (3,)    → 网络会报错
    #    ✓ 正确：[[80, 10, 70]]      shape: (1, 3)  → 表示 1 个样本
    #
    #    多个样本的情况：
    #    [[80, 10, 70],        shape: (2, 3)  → 表示 2 个样本
    #     [50, 50, 50]]
    #
    # 三、dtype=torch.float32 的重要性
    #
    #    Python 列表中的整数默认转换为 int64：
    #        torch.tensor([[80, 10, 70]])  → dtype=torch.int64
    #
    #    但神经网络的权重是 float32：
    #        model.fc1.weight.dtype  → torch.float32
    #
    #    类型不匹配会报错：
    #        RuntimeError: expected scalar type Float but found Long
    #
    #    解决方案：显式指定 dtype=torch.float32
    #
    # 四、test_sample 的具体内容
    #
    #    test_sample = tensor([[80., 10., 70.]])
    #    test_sample.shape = torch.Size([1, 3])
    #    test_sample.dtype = torch.float32
    #
    #    对应的气象条件：
    #        test_sample[0, 0] = 80.0  → 云量：很厚的云
    #        test_sample[0, 1] = 10.0  → 风速：微风
    #        test_sample[0, 2] = 70.0  → 湿度：比较潮湿
    #
    # ═══════════════════════════════════════════════════════════════════════
    #                          云=80  风=10  湿=70
    # ⚠️ 必须指定 dtype=torch.float32！
    #    否则 Python 列表里的整数会被转成 int64 (LongTensor)
    #    会导致 "RuntimeError: expected scalar type Float but found Long"
    
    # 预测
    prediction = model(test_sample).item()  # 得到概率
    #
    # ═══════════════════════════════════════════════════════════════════════
    # 【预测过程的详细说明】
    # ═══════════════════════════════════════════════════════════════════════
    #
    # 一、model(test_sample) 的返回值
    #
    #    输入：test_sample     shape: (1, 3)
    #    输出：model(...)      shape: (1, 1)
    #
    #    返回的是一个 Tensor，不是 Python 数字：
    #        model(test_sample) = tensor([[0.9234]])
    #
    # 二、.item() 的作用
    #
    #    将 Tensor 中的单个值提取为 Python float：
    #
    #        tensor([[0.9234]]).item() = 0.9234
    #
    #    注意：.item() 只能用于包含单个元素的 Tensor
    #        tensor([0.1, 0.2]).item()  → 报错！
    #        tensor(0.5).item()         → 0.5 ✓
    #        tensor([[0.5]]).item()     → 0.5 ✓
    #
    # 三、预测结果的含义
    #
    #    prediction ≈ 0.92 表示：
    #    - 模型认为有 92% 的概率会下雨
    #    - 因为 0.92 >= 0.5，所以最终预测为"下雨"
    #
    # 四、与真实规则的对比
    #
    #    我们设定的规则：湿度 + 云量 > 风速 × 1.2
    #
    #    对于 test_sample [80, 10, 70]：
    #        70 + 80 = 150
    #        10 × 1.2 = 12
    #        150 > 12 → True → 应该下雨
    #
    #    模型预测：0.92 → 下雨
    #    规则结果：True → 下雨
    #    结论：预测正确 ✓
    #
    # ═══════════════════════════════════════════════════════════════════════
    
    # 打印结果
    print("   📊 测试样本:")
    print(f"      云量: 80（很厚的云）")
    print(f"      风速: 10（微风）")
    print(f"      湿度: 70（比较潮湿）")
    print()
    
    # 用我们编造的规则验证
    # 规则：湿度 + 云量 > 风速 × 1.2
    rule_result = 70 + 80 > 10 * 1.2  # 150 > 12，True
    print(f"   🔍 根据规则: 湿度(70) + 云量(80) = 150 > 风速(10) × 1.2 = 12")
    print(f"      规则判断: {'下雨' if rule_result else '不下雨'}")
    print()
    
    print(f"   🤖 神经网络预测:")
    print(f"      下雨概率: {prediction:.4f} ({prediction*100:.1f}%)")
    print(f"      预测结果: {'🌧️ 下雨' if prediction >= 0.5 else '☀️ 不下雨'}")
    print()
    
    # 多测几个样本
    print("   📋 更多测试样本:")
    test_cases = [
        [10, 90, 20],   # 云少、风大、干燥 → 应该不下雨
        [90, 10, 90],   # 云厚、风小、潮湿 → 应该下雨
        [50, 50, 50],   # 中等情况 → 边界
    ]
    #
    # ═══════════════════════════════════════════════════════════════════════
    # 【测试用例的设计思路】
    # ═══════════════════════════════════════════════════════════════════════
    #
    # 一、测试用例的分析
    #
    #    规则：湿度 + 云量 > 风速 × 1.2
    #
    #    用例 1：[10, 90, 20]（云=10, 风=90, 湿=20）
    #        左边 = 20 + 10 = 30
    #        右边 = 90 × 1.2 = 108
    #        30 > 108? → False → 不下雨
    #        生活解读：云少、风大、干燥，显然不会下雨
    #
    #    用例 2：[90, 10, 90]（云=90, 风=10, 湿=90）
    #        左边 = 90 + 90 = 180
    #        右边 = 10 × 1.2 = 12
    #        180 > 12? → True → 下雨
    #        生活解读：云厚、风小、潮湿，很可能下雨
    #
    #    用例 3：[50, 50, 50]（云=50, 风=50, 湿=50）
    #        左边 = 50 + 50 = 100
    #        右边 = 50 × 1.2 = 60
    #        100 > 60? → True → 下雨
    #        生活解读：中等条件，偏向下雨（边界情况）
    #
    # 二、这些用例的意义
    #
    #    1. 极端情况测试：
    #       - 用例 1：明显不下雨
    #       - 用例 2：明显下雨
    #       → 验证模型在简单情况下的判断能力
    #
    #    2. 边界情况测试：
    #       - 用例 3：所有特征都为中值
    #       → 验证模型在模糊地带的表现
    #
    # 三、为什么要用列表存储测试用例？
    #
    #    方便用 for 循环批量测试：
    #
    #    for cloud_t, wind_t, humidity_t in test_cases:
    #        # 每次循环，解包出一个测试样本
    #        # cloud_t = 10, wind_t = 90, humidity_t = 20  (第一轮)
    #        # cloud_t = 90, wind_t = 10, humidity_t = 90  (第二轮)
    #        # cloud_t = 50, wind_t = 50, humidity_t = 50  (第三轮)
    #
    # ═══════════════════════════════════════════════════════════════════════
    
    for cloud_t, wind_t, humidity_t in test_cases:
        sample = torch.tensor([[cloud_t, wind_t, humidity_t]], dtype=torch.float32)
        prob = model(sample).item()
        rule = (humidity_t + cloud_t) > wind_t * 1.2
        #
        # ═══════════════════════════════════════════════════════════════════
        # 【这四行代码的详细解析】
        # ═══════════════════════════════════════════════════════════════════
        #
        # 1. for 循环解包
        #    每次迭代从 test_cases 取出一个列表 [c, w, h]
        #    Python 自动解包到三个变量：
        #        cloud_t, wind_t, humidity_t = [10, 90, 20]
        #    等价于：
        #        cloud_t = 10
        #        wind_t = 90
        #        humidity_t = 20
        #
        # 2. sample 的构建
        #    将三个变量组装成网络输入格式：
        #    sample = tensor([[10., 90., 20.]])
        #    shape: (1, 3) → 1 个样本，3 个特征
        #
        # 3. prob 的计算
        #    prob = model(sample).item()
        #    过程：
        #        model(sample)    → tensor([[0.0312]])  # 神经网络前向传播
        #        .item()          → 0.0312              # 提取数值
        #    prob = 0.0312 表示模型认为有 3.12% 概率下雨
        #
        # 4. rule 的计算
        #    rule = (humidity_t + cloud_t) > wind_t * 1.2
        #    过程（以第一个用例为例）：
        #        humidity_t + cloud_t = 20 + 10 = 30
        #        wind_t * 1.2 = 90 * 1.2 = 108
        #        30 > 108 → False
        #    rule = False 表示根据规则不应该下雨
        #
        # ═══════════════════════════════════════════════════════════════════
        #
        print(f"      云={cloud_t:2d}, 风={wind_t:2d}, 湿={humidity_t:2d}  →  "
              f"概率: {prob:.2f}  预测: {'🌧️' if prob >= 0.5 else '☀️'}  "
              f"规则: {'🌧️' if rule else '☀️'}  "
              f"{'✅' if (prob >= 0.5) == rule else '❌'}")
        #
        # ═══════════════════════════════════════════════════════════════════
        # 【输出格式化详解】
        # ═══════════════════════════════════════════════════════════════════
        #
        # f-string 格式化语法说明：
        #
        # {cloud_t:2d}
        #   │     │└─ d: 整数格式
        #   │     └─── 2: 最小宽度 2 位，不足补空格
        #   └───────── cloud_t: 要格式化的变量
        #
        #   示例：
        #       cloud_t = 5   → " 5"  (前面补空格)
        #       cloud_t = 90  → "90"  (刚好 2 位)
        #
        # {prob:.2f}
        #   │    │└─ f: 浮点数格式
        #   │    └─── .2: 保留 2 位小数
        #   └──────── prob: 要格式化的变量
        #
        #   示例：
        #       prob = 0.0312  → "0.03"
        #       prob = 0.9876  → "0.99"
        #
        # 三元表达式：'🌧️' if prob >= 0.5 else '☀️'
        #   如果 prob >= 0.5，返回 🌧️（下雨）
        #   否则返回 ☀️（晴天）
        #
        # 最后的结果判断：(prob >= 0.5) == rule
        #   比较模型预测和规则结果是否一致：
        #   - prob >= 0.5: 模型预测"下雨" (True) 或"不下雨" (False)
        #   - rule: 规则结果 True/False
        #   - 两者相等：预测正确 ✅
        #   - 两者不等：预测错误 ❌
        #
        # ═══════════════════════════════════════════════════════════════════

# ============================================================================
# 📝 练习题
# ============================================================================
"""
🏋️ 动手练习：

1. 【简单】修改隐藏层神经元数量
   - 把 fc1 的输出从 8 改成 16，观察效果
   - 思考：更多神经元一定更好吗？

2. 【中等】添加一层隐藏层
   - 在 fc1 和 fc2 之间再加一层 nn.Linear(8, 8)
   - 记得也要加激活函数！

3. 【中等】尝试不同的激活函数
   - 把隐藏层的 ReLU 换成 Tanh 或 LeakyReLU
   - 对比训练效果

4. 【进阶】修改下雨规则，让它更复杂
   - 比如：湿度² + 云量 > 风速 × 温度
   - 看看神经网络能不能学会

5. 【进阶】分离训练集和测试集
   - 用 80% 数据训练，20% 数据测试
   - 防止"死记硬背"（过拟合）

💡 提示：
   - 修改后重新运行，对比 Loss 曲线
   - 准确率不是越高越好，要看测试集准确率
"""

# ============================================================================
# 📚 知识点总结
# ============================================================================
"""
🔑 本课重点回顾：

1. 神经网络 = 层的堆叠
   - 每层做两件事：线性变换 + 非线性激活
   - 没有激活函数，再多层也等于一层

2. 训练四部曲（每个 epoch）：
   前向传播 → 计算损失 → 反向传播 → 更新权重

3. 常见坑：
   ⚠️ 忘记 optimizer.zero_grad()
   ⚠️ 输入输出维度不匹配
   ⚠️ 激活函数选错（分类用 Sigmoid/Softmax，回归用线性）

4. 超参数选择（需要调试）：
   - 隐藏层大小：太小欠拟合，太大过拟合
   - 学习率：太大震荡，太小收敛慢
   - epoch 数：太少没学会，太多过拟合

5. PyTorch 核心类：
   - nn.Module：网络基类
   - nn.Linear：全连接层
   - nn.BCELoss：二元交叉熵
   - optim.Adam：自适应优化器
"""

# print("\n" + "=" * 60)
# print("🎉 恭喜完成第一课！下一课：卷积神经网络 CNN")
# print("=" * 60)